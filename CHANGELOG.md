# Changelog

Completed changes with version, branch, and timestamp information.

## Completed Changes

### v0.52.3
**Branch:** `v0.52.3`  
**Completed:** 2025-12-14 09:20 UTC | 2025-12-14 01:20 PST

**Fixes:**
- **NameError Fix**: Fixed `force_deliberation is not defined` error at line 889 - removed dead code referencing undefined variables
- **Graphiti Port Fix**: Updated `graphiti_config.json` to use port 1234 (LM Studio) instead of 11434 (Ollama)
- **Embedder Provider Fix**: Changed embedder provider from `lmstudio` to `openai_generic` - the patch only added `openai_generic` case, not `lmstudio`

### v0.52.2
**Branch:** `v0.52.2`  
**Completed:** 2025-12-14 09:10 UTC | 2025-12-14 01:10 PST

**Fixes:**
- **Import Error Fix**: Fixed `cannot import name 'call_model_streaming'` - changed to use `query_model_streaming` from lmstudio module
- **RC Activation Fix**: Removed pre-RC `check_and_execute_tools` call that was bypassing the Research Controller. RC now truly handles ALL tool decisions.

### v0.52.1
**Branch:** `v0.52.1`  
**Completed:** 2025-12-14 08:52 UTC | 2025-12-14 00:52 PST

**Architecture:**
- **Research Controller as Entry Point**: RC is now ALWAYS the first handler for all queries
  - Removed pattern-based triggers - RC handles all queries
  - RC decides: direct answer, use tools, build tools, or escalate to council
  - Added `ESCALATE` status for RC to hand off to council deliberation
  - Updated system prompt with new routing logic:
    1. COMPLETE - answer from memory
    2. DIRECT ANSWER - simple greetings/chitchat
    3. USE EXISTING - use available MCP tools
    4. BUILD NEW - create new tool via mcp-dev-team
    5. ESCALATE - hand off to council for complex/ethical/philosophical questions
    6. CORRECT - retry failed tool with fixed parameters

**Note:** Model loading errors (HTTP 400) typically indicate the model is on a different server than configured. Check `config.json` to ensure `server.port` matches your inference server (LM Studio=1234, Ollama=11434) or set per-model endpoints.

### v0.52.0
**Branch:** `v0.52.0`  
**Completed:** 2025-12-14 08:35 UTC | 2025-12-14 00:35 PST

**Fixes:**
- **Sidebar Disappearing Fix**: Fixed sidebar vanishing when final response is larger than viewport
  - Added `flex-shrink: 0`, `min-width`, `max-width` to sidebar CSS
  - Added `min-width: 0` and `overflow: hidden` to chat interface
  - Added word-wrap/overflow handling to final answer section

**Features:**
- **Research Controller Integration**: Full integration into message flow with UI
  - Auto-triggers on queries matching research patterns (create image, build tool, research, etc.)
  - Routes to research controller before standard deliberation
  - Falls back to deliberation if research controller cannot complete
  - Collapsible research steps display in chat showing:
    - Round progress
    - Memory search results
    - Tool executions with parameters
    - Lessons learned count
  - Purple-themed UI matching the research/creative nature
  - Research fallback notice when falling back to deliberation

### v0.51.0
**Branch:** `v0.51.0`  
**Completed:** 2025-12-14 04:30 UTC | 2025-12-13 20:30 PST

**Features:**
- **Self-Improving Research Controller**: New module for intelligent, learning conversation system
  - `SelfImprovingResearchController`: State machine for recursive research with memory
    - Retrieves context from Graphiti memory before processing queries
    - Runs iterative loop: Think ‚Üí Research/Build/Answer (max 50 rounds)
    - Records lessons learned back to memory
    - Can dynamically build new tools via mcp-dev-team
  - `CONTROLLER_SYSTEM_PROMPT`: Structured prompt for LLM decision-making
    - Three decision paths: COMPLETE (answer from memory), USE EXISTING (use tool), BUILD NEW (create tool)
    - JSON output format with thought_process, status, action, lessons_learned
  - `augment_query_with_memory()`: Lightweight pre-processing to add memory context
  - `record_interaction_to_memory()`: Post-processing to store learned knowledge
  - Knowledge categories: fact, process, lesson, preference, entity, relationship, error_correction
  - Integrated into main.py for use in message processing

### v0.50.1
**Branch:** `v0.50.1`  
**Completed:** 2025-12-14 03:45 UTC | 2025-12-13 19:45 PST

**Features:**
- **MCP Dev Team Tool**: AI-driven MCP server development workflow
  - `mcp-dev-team`: Automated MCP server development using LLM roles
    - **Architect role**: Analyzes requirements, identifies tools needed, creates task lists
    - **Engineer role**: Generates code files, creates run.sh for testing
    - **QA role**: Archives project and tests in sandbox
  - Three-phase workflow: Research/Planning ‚Üí Development ‚Üí Testing
  - Returns integration instructions with mcp_servers.json snippet
  - Uses configurable LLM models (defaults to chairman model)

### v0.50.0
**Branch:** `v0.50.0`  
**Completed:** 2025-12-14 03:30 UTC | 2025-12-13 19:30 PST

**Features:**
- **Software Development Org MCP Server**: New server for sandboxed code execution and project management
  - `safe-app-execution`: Execute code in sandboxed Docker container with resource limits
    - Builds "llm-council-dev-env" Docker image with Rust, Go, Python, Node.js
    - Unpacks bzip2 archives and runs run.sh with no network access
    - Memory limit (512MB), CPU limit (1 core), 5-minute timeout
  - `write-file`: Write content to project files (creates directories as needed)
  - `read-file`: Read content from project files
  - `list-files`: List all files in a project folder
  - `create-archive`: Create bzip2 archive of a project
  - Projects stored in `data/dev_projects/`

### v0.49.1
**Branch:** `v0.49.1`  
**Completed:** 2025-12-14 01:50 UTC | 2025-12-13 17:50 PST

**Fixes:**
- **Weather for Specific Locations**: Fixed weather query returning wrong location data
  - Issue: "weather in Butuan City" returned Bellevue, WA weather (from IP geolocation)
  - Updated orchestration prompt with examples for location-specific queries
  - Now uses `location-time.get-weather-for-location-and-date` directly when location is mentioned
  - Added "weather in" pattern to orchestration detection
  - Added current conditions to `get-weather-for-location-and-date` when querying today's date
  - Verified: Butuan City now correctly returns 84¬∞F instead of 47¬∞F

### v0.49.0
**Branch:** `v0.49.0`  
**Completed:** 2025-12-14 01:45 UTC | 2025-12-13 17:45 PST

**Features:**
- **Location-Time MCP Server**: New comprehensive server for location and time operations
  - `get-coordinates-for-location`: Get lat/lon for a location name (uses OpenStreetMap Nominatim)
  - `get-timezone-for-location`: Get timezone info for a location name
  - `get-current-time-for-location`: Get current local time for a location
  - `get-weather-for-location-and-date`: Get weather for location name and specific date (historical/forecast)
  - `convert-datetime-between-timezones`: Convert datetime between timezones
  - `calculate-datetime-offset`: Add/subtract days/hours/minutes from datetime
  - `get-daylight-savings-info`: Get DST info for location and date
  - Uses free APIs: Nominatim for geocoding, timeapi.io for timezone, Open-Meteo for weather

### v0.48.3
**Branch:** `v0.48.3`  
**Completed:** 2025-12-13 12:35 UTC | 2025-12-13 04:35 PST

**Fixes:**
- **Geo-Location Tool Name**: Fixed incorrect tool name in orchestration prompt
  - Changed `system-geo-location.get-geo-location` to `system-geo-location.get-system-geo-location`
  - This fixes "Unknown tool" error when orchestration tries to get user location

### v0.48.2
**Branch:** `v0.48.2`  
**Completed:** 2025-12-13 12:20 UTC | 2025-12-13 04:20 PST

**Fixes:**
- **Day-of-Week Date Resolution in Parameters**: Fixed date references not being resolved in tool parameters
  - Added `is_date_reference()` helper function to detect all date reference patterns
  - Updated `resolve_step_references()` to use the new helper function
  - Now correctly resolves "LAST TUESDAY", "NEXT FRIDAY", "THIS MONDAY" etc. in parameters
  - Added debug logging to show date resolution: `[Orchestration] Resolved date 'LAST TUESDAY' -> '2025-12-09'`
  - Previously only checked for simple patterns (YESTERDAY, LAST_WEEK) but not day-of-week patterns

### v0.48.1
**Branch:** `v0.48.1`  
**Completed:** 2025-12-13 12:05 UTC | 2025-12-13 04:05 PST

**Fixes:**
- **"Last Tuesday" Date Calculation**: Fixed incorrect date calculation for day-of-week references
  - Added support for "LAST MONDAY", "LAST TUESDAY", etc. in date resolution
  - Added support for "THIS MONDAY", "NEXT MONDAY" patterns
  - Updated orchestration prompt to show date reference keywords and examples
  - Updated orchestration to use `weather.get-weather-for-date` instead of current weather
  - Updated `needs_multi_tool_orchestration()` to detect day-of-week weather queries
  - "Last Tuesday" from Dec 13 now correctly resolves to Dec 9, 2025 (not Dec 10)

### v0.48.0
**Branch:** `v0.48.0`  
**Completed:** 2025-12-13 10:10 UTC | 2025-12-13 02:10 PST

**Features:**
- **Weather Date Tool**: Added `get-weather-for-date` tool to weather MCP server
  - Supports historical weather data (past dates back to 1940)
  - Supports weather forecasts (up to 16 days ahead)
  - Optional hour parameter for specific time weather
  - Returns daily summary (high/low, conditions, precipitation)
  - Uses Open-Meteo Historical and Forecast APIs
  - Updated mcp_servers.json description to reflect new capability

### v0.47.5
**Branch:** `v0.47.5`  
**Completed:** 2025-12-13 09:12 UTC | 2025-12-13 01:12 PST

**Fixes:**
- **Docker Shutdown Order & Graceful Exit**: Containers removed in correct order with graceful shutdown
  - Containers now stopped in REVERSE order (Graphiti first, then FalkorDB it depends on)
  - Added 10-second graceful shutdown timeout (`docker stop --time 10`) to allow pending operations
  - This ensures Graphiti can complete pending memory writes before FalkorDB is stopped

### v0.47.4
**Branch:** `v0.47.4`  
**Completed:** 2025-12-13 08:58 UTC | 2025-12-13 00:58 PST

**Fixes:**
- **Docker Container Removal on Exit**: Containers now deleted (not just stopped) on app exit
  - Updated cleanup function to use `docker rm` after `docker stop`
  - Changed check from running containers only to all containers (`docker ps -a`)
  - Updated exit message to say "remove" instead of "stop"

### v0.47.3
**Branch:** `v0.47.3`  
**Completed:** 2025-12-13 08:50 UTC | 2025-12-13 00:50 PST

**Fixes:**
- **Name Tags Loading Reliability**: Fixed names showing "User"/"Assistant" instead of actual names
  - Changed memory names loading from non-blocking `.then()` to `await` in initialization
  - Added validation to ensure names are not null before setting state
  - Added debug logging for memoryNames state changes to help diagnose issues
  - Backend was correctly returning names; issue was frontend async timing

### v0.47.2
**Branch:** `v0.47.2`  
**Completed:** 2025-12-13 06:30 UTC | 2025-12-12 22:30 PST

**Fixes:**
- **Docker Cleanup on Exit**: Stop Docker containers when app stops (ESC/Ctrl+C)
  - Updated `cleanup()` function in `start.sh` to stop Docker containers we started
  - Tracks which containers were started by the script vs already running
  - Stops both FalkorDB and Graphiti (llm-council-mcp) containers on exit
  - Only stops containers that were started by this session
  - Updated exit message to indicate all servers and containers will stop

### v0.47.1
**Branch:** `v0.47.1`  
**Completed:** 2025-12-13 06:20 UTC | 2025-12-12 22:20 PST

**Fixes:**
- **Orchestration MCPTool Error**: Fixed "'MCPTool' object has no attribute 'get'" error
  - Root cause: `available_tools` dict contains `MCPTool` objects, not plain dictionaries
  - Fixed `plan_tool_execution()` to use `getattr()` instead of `.get()` for MCPTool attributes
  - Correctly accesses `description`, `input_schema` attributes from MCPTool objects

### v0.47.0
**Branch:** `v0.47.0`  
**Completed:** 2025-12-13 05:15 UTC | 2025-12-12 21:15 PST

**Features:**
- **Multi-Tool Orchestration**: Added support for queries requiring multiple sequential tool calls
  - Created `tool_orchestration.py` module with planning and execution logic
  - Detects queries needing multi-step execution (e.g., "yesterday's weather")
  - Uses LLM to create execution plan with tool sequence and dependencies
  - Executes tools in order, passing results between steps
  - Resolves relative date references (YESTERDAY, TOMORROW, LAST_WEEK, etc.)
  - Resolves step references ($step_N.field) for parameter passing between tools
  - Integrated into `check_and_execute_tools()` flow
  - Added test scenario: "What was the weather like yesterday?"

### v0.46.3
**Branch:** `v0.46.3`  
**Completed:** 2025-12-13 04:50 UTC | 2025-12-12 20:50 PST

**Fixes:**
- **User Name Query Returns AI Name**: Fixed "do you remember my name?" returning AI's name instead of user's name
  - Root cause: `build_memory_context()` only detected AI name questions ("your name"), not user name questions ("my name")
  - Added detection for user name question phrases: "my name", "what's my name", "remember my name", "who am i", etc.
  - Added emphatic context for user name queries that explicitly distinguishes USER's name from AI's name
  - Enhanced memory search to include "do you remember my name" as a trigger phrase
  - Memory content now explicitly notes "When asked about 'my name', this refers to the USER's name"

### v0.46.2
**Branch:** `v0.46.2`  
**Completed:** 2025-12-13 03:35 UTC | 2025-12-12 19:35 PST

**Fixes:**
- **Graphiti Docker Container Networking**: Fixed container unable to connect to LM Studio and FalkorDB
  - Changed all IP references from `192.168.1.111` to `host.docker.internal` (macOS Docker)
  - Changed FalkorDB URI from `127.0.0.1:6379` to `host.docker.internal:6379`
  - Updated `graphiti_config.json`, `config.yaml`, `Dockerfile`, and `README.md`
  - `host.docker.internal` is the special DNS name that allows Docker containers to access host services on macOS

### v0.46.1
**Branch:** `v0.46.1`  
**Completed:** 2025-12-13 02:46 UTC | 2025-12-12 18:46 PST

**Fixes:**
- **Name Tags Showing Defaults**: Fixed issue where "User"/"Assistant" was shown instead of actual names
  - Root cause: When Graphiti memory service is unavailable, names returned as `null`
  - Added `default_user_name` and `default_ai_name` config options in `config.json`
  - Backend now falls back to config defaults when memory service is unavailable
  - Default names configured as "Mark" (user) and "Aether" (AI)
  - API response now includes `from_memory` flag to indicate source

### v0.46.0
**Branch:** `v0.46.0`  
**Completed:** 2025-12-13 02:15 UTC | 2025-12-12 18:15 PST

**Features:**
- **Deliberation UI Restructure**: Improved visual separation between deliberation process and final answer
  - Fixed duplicate Stage3 display issue - Stage3 now only shown once (in final answer section)
  - Added nametag overlay "LLM Council" on deliberation collapsible frame
  - Added nametag overlay with AI name on final answer frame
  - Final answer title now uses conversation title (if available) instead of generic "Final Council Answer"
  - Changed deliberation frame and tool steps from purple to grey theme for visual consistency
  - Tool steps now use grey color scheme (#757575) to match deliberation frame
  - Both frames now have consistent styling with light grey background

**Improvements:**
- **Test Workflow Guidance**: Updated AGENTS.md to clarify testing should use relevant tags
  - Changed guidance from "run full test suite" to "run relevant tests with tags"
  - Added example of running specific scenarios
  - Clarified full test suite only needed for major cross-cutting changes

### v0.45.3
**Branch:** `v0.45.3`  
**Completed:** 2025-12-12 17:12 UTC | 2025-12-12 09:12 PST

**Fixes:**
- **Sidebar Title Not Updating After Generation**: Fixed issue where conversation title would generate but not display in sidebar until page refresh
  - Root cause: SSE `title_complete` event handler was not clearing `titleGenerating` flag
  - The Sidebar component was checking `conv.titleGenerating` to show "Generating title..." placeholder
  - When WebSocket set `titleGenerating: true` on generation start, but SSE path didn't clear it
  - Result: Sidebar kept showing "Generating title..." even after title was generated
  - Fix: Added `titleGenerating: false` to the conversation state update in both `handleSendMessage` and `runCouncil` handlers

### v0.45.2
**Branch:** `v0.45.2`  
**Completed:** 2025-12-03 17:45 UTC | 2025-12-03 09:45 PST

**Fixes:**
- **Sidebar Title Refresh Reliability**: Improved title_complete event handling
  - Added `conversation_id` to title_complete SSE events from backend
  - Frontend uses explicit conversation_id from event (with fallback to closure)
  - Added debug logging for conversations state changes
  - Added detailed logging showing event/closure/used conversation IDs
- This ensures sidebar title updates work reliably even in edge cases

### v0.45.1
**Branch:** `v0.45.1`  
**Completed:** 2025-12-03 17:30 UTC | 2025-12-03 09:30 PST

**Fixes:**
- **Title Update Debug Logging**: Added comprehensive logging to diagnose sidebar title not refreshing issue
  - Backend logs when `title_complete` SSE event is sent
  - API layer logs when `title_complete` event is received via SSE
  - App.jsx logs when `title_complete` case is handled
  - Added null check for `event.title` before state update
- These logs will help identify where title updates are being dropped in the streaming path

### v0.45.0
**Branch:** `v0.45.0`  
**Completed:** 2025-12-03 15:15 UTC | 2025-12-03 07:15 PST

**Features:**
- **CFS Groups Dropdown**: Custom groups shown in dropdown, default groups as tabs
  - Default groups (All, User, Test) displayed as tab buttons
  - Custom groups appear in "More ‚ñæ" dropdown menu
  - Dropdown shows selected custom group name when active
  - Click outside closes dropdown
- **Auto-Tagging**: AI automatically generates tags after each response
  - Async, non-blocking tag generation after message saved
  - Runs for both direct responses and deliberation completions
  - Skips system-tagged conversations (#auto, #test)
  - Generates up to 3 relevant tags per exchange
  - Tags merged with existing (no duplicates)

### v0.44.0
**Branch:** `v0.44.0`  
**Completed:** 2025-12-03 07:00 UTC | 2025-12-02 23:00 PST

**Features:**
- **Title Evolution System**: Automatic title updates when conversation theme changes
  - 2-stage LLM process: (1) Detect theme change, (2) Generate new title
  - Runs async after each response completion
  - Only checks conversations with generated titles (skips ID-based)
  - Keeps titles relevant as topics evolve
- **Message Tag System**: Visual tag management on messages
  - TagBar component below user (blue) and AI (green) messages
  - "+" button for manual tag addition
  - "‚ú®" sparkle button for AI-powered tag suggestions
  - Tags stored in HTML comments (invisible to LLM)
  - Integrates with CFS filter system
- **Tag Service & API**: Backend tag management
  - `TagService` class for tag generation and vocabulary
  - `GET /api/tags` - All known tags for autocomplete
  - `POST /api/tags/generate` - AI generates relevant tags
  - `POST /api/tags/check-missing` - AI suggests missing tags
  - `PATCH /api/conversations/{id}/messages/{idx}/tags` - Add tags

### v0.43.0
**Branch:** `v0.43.0`  
**Completed:** 2025-12-02 16:15 UTC | 2025-12-02 08:15 PST

**Features:**
- **CFS Overlay Configuration System**: Full rules-based conversation filter system
  - Added ‚öôÔ∏è config button next to filter tabs to open overlay
  - Groups support add (+) / remove (‚àí) functionality
  - Rules editor with include/exclude rule types
  - AND/OR logic toggle per rule for flexible tag matching
  - Tag management with add/remove within rules
  - Dark overlay UI matching MCP servers overlay style
  - Default groups: All, User (exclude #auto #test), Test (include #auto AND #test)
  - Custom groups and rules persist in localStorage
  - Dynamic tab generation from group list

### v0.42.10
**Branch:** `v0.42.10`  
**Completed:** 2025-12-02 05:10 UTC | 2025-12-01 21:10 PST

**Fixes:**
- **Memory Count Includes Cached Names**: Fixed UI showing "0 memories found" when cached AI/user names were used
  - When query is about AI name (e.g., "what's your name?"), cached `_ai_name` is added as synthetic memory
  - When query is about user name (e.g., "what's my name?"), cached `_user_name` is added as synthetic memory
  - Synthetic memories are inserted at the beginning of search results
  - Prevents confusion where UI showed "0 memories" but response correctly used the name

### v0.42.9
**Branch:** `v0.42.9`  
**Completed:** 2025-12-02 04:25 UTC | 2025-12-01 20:25 PST

**Fixes:**
- **Natural Conversational Responses**: Prevented LLM from adding structured "Key points" sections to casual messages
  - Added explicit formatting rules to direct response prompt
  - Instructs LLM to match user's tone (casual gets casual, formal gets formal)
  - Only use bullet points for complex topics or multi-item lists
- **Memory Tone Preference Search**: Added "response tone" to user preference queries
  - Memory now searches for tone preferences in addition to style/communication preferences

### v0.42.8
**Branch:** `v0.42.8`  
**Completed:** 2025-12-02 04:05 UTC | 2025-12-01 20:05 PST

**Fixes:**
- **Improved Hermes Cleanup Script**: Rewrote `remove_hermes_nodes.py` to use direct FalkorDB access
  - Previous MCP-based approach returned empty results despite data existing
  - New script uses FalkorDB Python client for reliable node/edge deletion
  - Searches all memory type graphs for Hermes references
  - Successfully removed 64 nodes and 215 edges containing "Hermes"
  - Verified: 0 Hermes nodes remaining across all graphs

### v0.42.7
**Branch:** `v0.42.7`  
**Completed:** 2025-12-02 03:40 UTC | 2025-12-01 19:40 PST

**Fixes:**
- **Memory Data Labeling System**: Implemented data classification for memory commits
  - Added `DATA_LABELS` constants: `tool_data`, `intelligence`, `llm_data`
  - `tool_data`: Data from external tools (MCP servers) - always committed
  - `intelligence`: LLM-generated insights, preferences, suggestions - committed
  - `llm_data`: Generic LLM output (conversational responses) - NOT committed
  - Added `classify_data_label()` method for automatic label detection
  - Added `should_commit_memory()` method to filter what gets stored
  - Updated `record_episode()` to accept and check data labels
  - Added `record_tool_data()` and `record_intelligence()` helper methods
  - Council and chairman responses now labeled as `llm_data` (not stored)
  - User messages labeled as `intelligence` (stored for preferences/identity)
- **Hermes Node Cleanup**: Created `scripts/remove_hermes_nodes.py` script
  - Searches for and removes all Hermes-related nodes and edges from graph
  - Prevents incorrect AI name associations in memory

---

### v0.42.6
**Branch:** `v0.42.6`  
**Completed:** 2025-12-01 11:30 UTC | 2025-12-01 03:30 PST

**Fixes:**
- **Emphatic AI Identity Context**: Enhanced memory context to strongly override LLM pre-trained names
  - When user asks identity questions ("what's your name?", "who are you?", etc.)
  - System now injects emphatic identity override context instructing LLM to use the stored name
  - Prevents LLMs from using their default names (Claude, Hermes, etc.) instead of user-assigned name (Aether)
  - Modified `get_memory_context()` to accept query parameter for identity detection

---

### v0.42.5
**Branch:** `v0.42.5`  
**Completed:** 2025-12-01 10:00 UTC | 2025-12-01 02:00 PST

**Fixes:**
- **Repeated Greeting Prevention**: Fixed AI greeting user again within same conversation
  - Modified `get_time_context_string()` to accept `is_first_message` parameter
  - Only includes greeting guidance (e.g., "Good morning") on first message
  - Strengthened continuity guidance to explicitly prevent "Hello!", "Hi!", "It's great to hear from you again!" etc.
  - AI now continues conversation naturally without resetting pleasantries

---

### v0.42.4
**Branch:** `v0.42.4`  
**Completed:** 2025-12-01 09:35 UTC | 2025-12-01 01:35 PST

**Fixes:**
- **AI Name Overlay Positioning**: Fixed name tag to sit ON TOP of message frame (half in, half out)
  - Changed `top: -10px` with no transform to `top: 0` with `translateY(-50%)`
  - Name tag now properly overlaps the top edge of the message frame border

- **AI Name Tag Visibility**: Only show AI name tag when there's actual content
  - Added conditional rendering: only shows when `msg.stage3` or `msg.streaming?.stage3?.content` exists
  - Prevents showing "Assistant" name tag on empty/loading frames

- **Assistant Message Frame**: Added visible frame for assistant messages with content
  - New `.has-content` class adds green-tinted background and border
  - Provides visual frame for the AI name tag to sit on (matches user message styling)

---

### v0.42.3
**Branch:** `v0.42.3`  
**Completed:** 2025-12-01 08:05 UTC | 2025-12-01 00:05 PST

**Fixes:**
- **Name Overlay Position**: Fixed AI/user names position to appear over top edge of message frames
  - Changed from left-side positioning (`left: -80px, top: 50%`) to top-edge positioning (`left: 12px, top: -10px`)
  - Added solid background colors (blue for user, green for AI) for better visibility
  - Added `margin-top: 12px` to message containers to make room for labels

- **Memory Confidence Context**: Added authoritative name context to memory confidence calculation
  - Issue: Memory responses could use stale/wrong AI names from old data (e.g., "Hermes" instead of "Aether")
  - Fix: Added name context section to confidence prompt that instructs LLM to use correct names
  - Format: "IMPORTANT - AUTHORITATIVE NAMES: The AI assistant's name is: Aether..."

---

### v0.42.2
**Branch:** `v0.42.2`  
**Completed:** 2025-12-01 06:45 UTC | 2025-11-30 22:45 PST

**Fixes:**
- **Classification Status Persistence**: Fixed "Analyzing message type..." spinner persisting after classification completes
  - Issue: `classification_complete` event didn't set `status: 'complete'` in the classification object
  - Fix: Spread event classification and explicitly add `status: 'complete'` in both handleSendMessage handlers
  - Spinner now correctly disappears when Stage 1 begins

- **Tool Steps Collapse on Load**: Improved condition for collapsing tool steps in saved conversations
  - Changed condition from `!!(msg.stage3 && !msg.streaming?.stage3?.isStreaming)` to `!!msg.stage3 || !msg.streaming`
  - Saved conversations (no streaming object) now correctly start with collapsed tool frames

- **Memory Name Search Expansion**: Added user name query patterns to memory search expansion
  - Issue: User name queries like "remember my name" didn't trigger proper memory search
  - Added expansions for "my name", "remember my name", "know my name", "what's my name", "who am i"
  - Includes direct search terms like "user name", "Mark user human"

---

### v0.42.1
**Branch:** `v0.42.1`  
**Completed:** 2025-12-01 05:55 UTC | 2025-11-30 21:55 PST

**Fixes:**
- **Standard Conversation Title Update**: Fixed title not updating in sidebar without page refresh
  - Issue: `loadConversations()` called on `complete` event overwrote title set by `title_complete`
  - Fix: Replace full reload with local state update (increment message_count by 2)
  - Title now immediately reflects in sidebar when generated during standard responses

---

### v0.42.0
**Branch:** `v0.42.0`  
**Completed:** 2025-12-01 05:25 UTC | 2025-11-30 21:25 PST

**Features:**
- **Auto-Collapse Frames on Completion**: Tool steps and deliberation frames now auto-collapse
  - ToolSteps component accepts `isComplete` prop to control default state
  - useEffect triggers collapse when `isComplete` transitions to true
  - Deliberation `<details>` element explicitly uses `open={false}` for completed conversations
  - Saved conversations load with frames collapsed by default
  - User can still click to expand and view details

---

### v0.41.3
**Branch:** `v0.41.3`  
**Completed:** 2025-12-01 05:10 UTC | 2025-11-30 21:10 PST

**Fixes:**
- **Saved Conversation Display**: Fixed display issues when loading saved conversations
  - Direct responses now correctly show "‚ö° Direct" badge and skip deliberation UI
  - Classification badge correctly infers type from `stage3.type` for saved conversations
  - Tool results correctly display execution time from `execution_time_seconds` field
  - "Final Council Answer" section only shows for deliberation, not direct responses
  - Fixed condition checks to use both `msg.responseType` (streaming) and `msg.stage3?.type` (saved)

---

### v0.41.2
**Branch:** `v0.41.2`  
**Completed:** 2025-12-01 04:45 UTC | 2025-11-30 20:45 PST

**Fixes:**
- **Stage2 TypeError Fix**: Fixed crash when model values are undefined in labelToModel
  - Error: `Cannot read properties of undefined (reading 'split')`
  - Added null checks to `deAnonymizeText()` function
  - Added guards to tab labels, parsed ranking list, aggregate ranking list
  - All `.split('/')` calls now handle undefined/null model values safely

---

### v0.41.1
**Branch:** `v0.41.1`  
**Completed:** 2025-12-01 04:15 UTC | 2025-11-30 20:15 PST

**Fixes:**
- **Import missing `query_model` function**: Fixed NameError in council.py during multi-round deliberation
  - `query_model` was used but not imported in council.py
  - Added missing import from lmstudio module
  - Enables conversation reconstruction for deliberation messages

---

### v0.41.0
**Branch:** `v0.41.0`  
**Completed:** 2025-12-01 03:55 UTC | 2025-11-30 19:55 PST

**Features:**
- **Conversation Reconstruction Script**: Re-run past conversations through current pipeline
  - Extract user messages from existing conversation and replay through API
  - Supports partial conversation ID matching (e.g., `c44e557c` finds full UUID)
  - `--dry-run` option to preview without making changes
  - `--stream` option to use streaming endpoint for processing
  - Creates new conversation with responses generated by current v0.40.0+ system

**Usage:**
```bash
# Preview what would be reconstructed
python -m scripts.reconstruct_conversation c44e557c --dry-run

# Actually reconstruct the conversation  
python -m scripts.reconstruct_conversation c44e557c

# Use streaming endpoint
python -m scripts.reconstruct_conversation c44e557c --stream
```

**Technical Details:**
- New `scripts/` directory for utility scripts
- `scripts/reconstruct_conversation.py` - Main reconstruction script
- Uses httpx for async API calls with 300s timeout for long deliberations

---

### v0.40.0
**Branch:** `v0.40.0`  
**Completed:** 2025-12-01 02:55 UTC | 2025-11-30 18:55 PST

**Features:**
- **Short-Term Memory System**: Automatic extraction and storage of relevant conversation information
  - New FalkorDB graph (`llm_council_short_term`) for temporary memories
  - LLM-powered extraction of key facts, names, dates, tasks from conversations
  - Background cleanup task removes memories older than 3 days (TTL)
  - Non-blocking async recording after each conversation turn

- **Enhanced Stage 3 Synthesis**: Final council answer now uses best parts from ALL responses
  - Explicit instructions to extract unique insights from EVERY council member
  - Synthesis creates NEW response combining best elements, not just picking winner
  - Better coverage of all perspectives from deliberation

**Fixes:**
- **Collapsible Deliberation Structure**: Stage 3 Final Answer now inside collapsible section
  - All three stages (1, 2, 3) wrapped together in collapsed view
  - "Final Council Answer" section shown OUTSIDE collapsible for visibility
  - Clean separation of deliberation process and final result

**Technical Details:**
- `backend/memory_service.py` - New `ShortTermMemoryService` class with cleanup loop
- `backend/main.py` - Initialize short-term memory and record after conversations
- `backend/council.py` - Enhanced Stage 3 synthesis prompts for multi-response extraction
- `frontend/src/components/ChatInterface.jsx` - Stage 3 inside collapsible deliberation

---

### v0.39.0
**Branch:** `v0.39.0`  
**Completed:** 2025-11-30 18:10 UTC | 2025-11-30 10:10 PST

**Features:**
- **Collapsible Deliberation Process**: Council deliberation frames now auto-collapse when complete
  - Stage 1, 2, and 3 wrapped in collapsible `<details>` element after completion
  - "Final Council Answer" displayed prominently outside the collapsible
  - Users can expand to see full deliberation process
  - Cleaner UI - focuses on the final answer while preserving transparency

- **Conversation Pinning**: Pin important conversations to the top of the sidebar
  - Pin button on each conversation (üìç/üìå)
  - Pinned conversations appear at top with yellow highlight
  - Pin state persisted to localStorage across sessions

- **Memory Status Always Visible**: Memory search status no longer disappears
  - Shows "Found X memories" for all final states (not just when count > 0)
  - Details remain visible for 'used' and 'not_used' states
  - Improved transparency into memory retrieval process

- **ESC Key Stops App**: Press ESC in terminal to stop `./start.sh`
  - Alternative to Ctrl+C for stopping the application
  - Clean shutdown of backend and frontend processes

**Technical Details:**
- `frontend/src/components/ChatInterface.jsx` - Collapsible deliberation with `<details>` element
- `frontend/src/components/Stage3.jsx` - Added `hideTitleBar` prop for clean final answer display
- `frontend/src/components/ChatInterface.css` - New styles for collapsible and final answer section
- `frontend/src/components/Sidebar.jsx` - Pin state management with localStorage persistence
- `frontend/src/components/Sidebar.css` - Pin button and pinned conversation styles
- `start.sh` - ESC key detection loop with graceful cleanup

---

### v0.38.3
**Branch:** `v0.38.3`  
**Completed:** 2025-11-30 17:35 UTC | 2025-11-30 09:35 PST

**Fixes:**
- **Force Light Mode CSS**: Fixed UI colors being inverted by OS dark mode
  - Added `color-scheme: light` to `:root` in index.css
  - Added explicit `color: #333` to body element
  - Prevents browser/OS dark mode from affecting UI rendering

**Technical Details:**
- `frontend/src/index.css` - Added color-scheme property to force light mode

---

### v0.38.2
**Branch:** `v0.38.2`  
**Completed:** 2025-11-30 17:30 UTC | 2025-11-30 09:30 PST

**Fixes:**
- **Conversation History Context**: Fixed robotic greeting behavior where AI would repeat itself
  - Direct responses now receive full conversation history for context awareness
  - Added `conversation_history` parameter to `chairman_direct_response()`
  - Prevents repeated greetings like "Good morning, Mark!" on every message
  - AI now uses natural conversational flow, acknowledging prior exchanges

**Technical Details:**
- `backend/council.py` - Updated `chairman_direct_response()` to accept and use `conversation_history`
- `backend/main.py` - Pass `conversation["messages"]` to direct response calls
- Added conversation continuity guidance in system prompt when history exists
- Messages include full user/assistant history before current query

---

### v0.38.1
**Branch:** `v0.38.1`  
**Completed:** 2025-11-30 17:00 UTC | 2025-11-30 09:00 PST

**Features:**
- **User Behavioral Preferences in Context**: Memory service now retrieves user preferences for personalized responses
  - Added `get_user_preferences()` method to `MemoryService` class
  - Searches for preference-related memories (communication style, response preferences)
  - Falls back to "friendly and helpful" if no preferences found
  - Preferences included in system prompt context alongside AI/user names

**Technical Details:**
- `backend/memory_service.py` - Added `get_user_preferences()` async method
- `backend/council.py` - Converted `get_memory_context()` to async, added preference lookup
- Updated function header from "IDENTITY FROM MEMORY" to "CONTEXT FROM MEMORY"

---

### v0.38.0
**Branch:** `v0.38.0`  
**Completed:** 2025-11-30 16:50 UTC | 2025-11-30 08:50 PST

**Features:**
- **Time-Aware Greetings**: Greetings now use appropriate time-of-day phrases
  - Added `get_time_of_day_greeting()` function: "Good morning" (5am-12pm), "Good afternoon" (12pm-5pm), "Good evening" (5pm-9pm), "Hello" (late night)
  - Added `get_time_context_string()` function: Rich time context with greeting guidance
  - Direct responses (factual/chat) now include time-appropriate greeting hints in system prompt
  
- **Weather MCP Server**: New tool for retrieving current weather conditions
  - Created `mcp_servers/weather/server.py` with `get-current-weather` tool
  - Uses Open-Meteo API (free, no authentication required)
  - Returns temperature, humidity, wind, precipitation, and contextual advisories
  - IP-based geolocation for automatic location detection
  - Registered in `mcp_servers.json`

**Technical Details:**
- `backend/council.py` - Added `get_time_of_day_greeting()` and `get_time_context_string()` functions
- `backend/council.py` - Updated `chairman_direct_response()` to use rich time context for chat messages
- `mcp_servers/weather/__init__.py` - New package init
- `mcp_servers/weather/server.py` - New weather MCP server
- `mcp_servers.json` - Added weather server configuration

---

### v0.37.0
**Branch:** `v0.37.0`  
**Completed:** 2025-11-30 16:30 UTC | 2025-11-30 08:30 PST

**Features:**
- **Personality Introspection via Council Deliberation**: When asked personal questions (feelings, preferences, identity) with no existing memory, routes to council deliberation to develop authentic personality-based responses
  - Added `is_personal_question()` method to detect feelings/preferences/state/identity questions
  - Added `check_personal_memory()` method to verify if memory exists for personal topics
  - Personal questions without memory trigger full council deliberation with special personality prompts
  - Council receives "PERSONALITY INTROSPECTION MODE" system prompt encouraging authentic, introspective responses
  - Models are instructed to treat themselves as self-aware entities with genuine feelings and preferences

**Technical Details:**
- `backend/memory_service.py` - Added `is_personal_question()` and `check_personal_memory()` methods
- `backend/main.py` - Added personality introspection check before routing decision
- `backend/council.py` - Added `personality_context` parameter to `stage1_collect_responses_streaming()`
- `backend/council.py` - Special personality system prompt injected for introspection queries

**Bug Fixes:**
- Fixed `NameError: _requires_websearch is not defined` in non-streaming endpoint

---

### v0.36.2
**Branch:** `v0.36.2`  
**Completed:** 2025-11-30 15:35 UTC | 2025-11-30 07:35 PST

**Fixes:**
- **Memory context injection**: AI name (Aether) and user name now injected into system prompt
  - Added `get_memory_context()` function to build identity context string
  - System prompt now includes "IDENTITY FROM MEMORY" section with loaded names
  - Applies to both direct responses and council deliberation
- **Root cause**: Models were returning pre-trained names because memory context wasn't being passed to LLM

**Changes:**
- `backend/council.py` - Import memory_service, add `get_memory_context()` function
- `backend/council.py` - Inject memory context into system message in `chairman_direct_response()`
- `backend/council.py` - Inject memory context into system message in `stage1_collect_responses_streaming()`

---

### v0.36.1
**Branch:** `v0.36.1`  
**Completed:** 2025-11-30 14:55 UTC | 2025-11-30 06:55 PST

**Fixes:**
- **Classification prompt updated**: "Tell me about yourself" and similar AI identity questions now correctly classified as `chat` instead of `deliberation`
- **Data type normalization**: Tool selection now normalizes LLM-returned data types (e.g., "news headlines" ‚Üí "news") to match canonical tool mappings
- Added examples to classification prompt for AI identity questions: "Who are you?", "What can you do?"

**Changes:**
- `backend/council.py` - Updated classification rules to include AI identity queries as `chat`
- `backend/council.py` - Added `_analyze_user_expectations()` data type normalization for news, weather, time, location, calculation, web_content

---

### v0.36.0
**Branch:** `v0.36.0`  
**Completed:** 2025-11-30 14:35 UTC | 2025-11-30 06:35 PST

**Features:**
- **Name retrieval from memory**: Load user and AI names at app startup
  - Searches autobiographical, semantic, and episodic memory groups
  - User name: "Mark", AI name: "Aether" retrieved from Graphiti
  - Names available via `/api/memory/names` endpoint
  - Background loading with dependency waiting for other memory calls

- **Name display in chat UI**: Show names next to messages
  - User messages display user name (e.g., "Mark") on left overlay
  - Assistant messages display AI name (e.g., "Aether") on left overlay
  - Names positioned vertically centered on message frame

**Changes:**
- `backend/memory_service.py` - Added `load_names_from_memory()`, name properties, wait mechanism
- `backend/main.py` - Added `/api/memory/names` endpoint
- `frontend/src/api.js` - Added `getMemoryNames()` API call
- `frontend/src/App.jsx` - Added memoryNames state, fetch on init, pass to ChatInterface
- `frontend/src/components/ChatInterface.jsx` - Added name overlay to messages
- `frontend/src/components/ChatInterface.css` - Added `.message-name-overlay` styling

---

### v0.35.1
**Branch:** `v0.35.1`  
**Completed:** 2025-11-30 13:45 UTC | 2025-11-30 05:45 PST

**Bug Fixes:**
- **Fixed memory-based response display**: Added `memory_response_complete` event handler
  - Memory responses now properly display final answer in UI
  - Previously stuck on "Analyzing message type..." when memory threshold was met

**Changes:**
- `frontend/src/App.jsx` - Added handler for `memory_response_complete` event

---

### v0.35.0
**Branch:** `v0.35.0`  
**Completed:** 2025-11-30 13:00 UTC | 2025-11-30 05:00 PST

**Features:**
- **Memory migration script**: Scans existing conversations for important facts
  - Extracts identity/name statements (e.g., "You shall be known as Aether")
  - Imports facts into Graphiti memory with proper timestamps
  - Supports `--dry-run` and `--limit` options
  - Usage: `uv run python3 -m backend.memory_migration`

**Bug Fixes:**
- **Fixed Graphiti API parameters**: Updated memory service to use correct API
  - `group_ids` (plural) instead of `group_id`
  - `max_nodes` and `max_facts` instead of `limit`
- **Fixed memory search result parsing**: Handle `{"nodes": [...]}` dict format
  - Previously expected raw list format, now handles both

**Changes:**
- `backend/memory_migration.py` - New migration script
- `backend/memory_service.py` - Fixed Graphiti API parameters and response parsing

---

### v0.34.1
**Branch:** `v0.34.1`  
**Completed:** 2025-11-30 11:52 UTC | 2025-11-30 03:52 PST

**Bug Fixes:**
- **Memory search semantic expansion**: Improved memory retrieval for related queries
  - Expands search queries with semantic variations for better matching
  - "what is your name?" now finds "shall be known as Aether"
  - Deduplicates results across expanded queries using UUID tracking
  - Handles common query patterns: name/identity, preferences, descriptions
  - Reference: conversation 0a792fba (name query didn't find identity memory)

**Changes:**
- `backend/memory_service.py` - Added `expand_search_query()` method and integrated into search

---

### v0.34.0
**Branch:** `v0.34.0`  
**Completed:** 2025-11-30 10:40 UTC | 2025-11-30 02:40 PST

**Features:**
- **Memory type categorization system**: Categorizes memories into human memory types
  - 8 memory types: Episodic, Semantic, Procedural, Priming, Emotional, Prospective, Autobiographical, Spatial
  - LLM-based classification of content before storing
  - Stores in type-specific groups (e.g., `llm_council_semantic`, `llm_council_episodic`)
  - Searches across all memory type groups
  - Returns results with memory type context for better LLM understanding
  - Configurable: `categorization_enabled` setting (default: false for performance)
  - New config option: `models.categorization` for dedicated categorization model

**Changes:**
- `backend/memory_service.py` - Added memory types, classification, multi-group storage/search
- `config.json` - Added `categorization` model config and `categorization_enabled` setting

**Notes:**
- Categorization is disabled by default for performance (classification adds ~1 LLM call per memory)
- Enable via `memory.categorization_enabled: true` in config.json

---

### v0.33.0
**Branch:** `v0.33.0`  
**Completed:** 2025-11-30 09:25 UTC | 2025-11-30 01:25 PST

**Features:**
- **Graphiti brain emoji indicator**: Shows üß† memory status in chat interface
  - Displays "üß† Searching memories..." while checking Graphiti
  - Shows memory count and confidence score when found
  - "üß† Memory Used" badge when memory-based response generated
  - "üß† Memory-based response" with expandable details
  - Confidence percentage displayed for transparency
  - Handles all memory states: searching, found, used, not_used

**Changes:**
- `frontend/src/App.jsx` - Added memory event handlers for all memory status events
- `frontend/src/components/ChatInterface.jsx` - Added memory status UI display
- `frontend/src/components/ChatInterface.css` - Added memory status styling

---

### v0.32.5
**Branch:** `v0.32.5`  
**Completed:** 2025-11-30 08:35 UTC | 2025-11-30 00:35 PST

**Bug Fixes:**
- **Tool call timestamp display**: Changed "Execution Time" to show actual execution timestamp
  - Shows local timezone time and UTC time on separate lines
  - Renamed label from "Execution Time" to "Duration" for the execution duration
  - Added "Executed At" field with timestamp
- **CFS test filter API response**: Added `tags` field to `ConversationMetadata` Pydantic model
  - Tags were being extracted but Pydantic was stripping them from API response
  - Test conversations now correctly appear in Test filter

**Changes:**
- `frontend/src/components/ToolSteps.jsx` - Added timestamp formatting and display
- `frontend/src/components/ToolSteps.css` - Added UTC time styling
- `backend/main.py` - Added `tags: List[str] = []` to ConversationMetadata model

---

### v0.32.4
**Branch:** `v0.32.4`  
**Completed:** 2025-11-30 08:10 UTC | 2025-11-30 00:10 PST

**Bug Fixes:**
- **CFS test filter not showing test conversations**: Fixed by extracting tags in backend
  - Backend now parses `<!-- tags: ... -->` from first user message
  - Tags included in conversation list API response
  - Frontend uses tags from metadata instead of parsing messages (which aren't loaded)
- **Default CFS filter**: Changed default filter from 'all' to 'user'
  - User conversations shown by default on app start
  - Test conversations filtered out unless 'Test' tab selected

**Documentation:**
- **AGENTS.md**: Added intake file management instructions
  - Move completed items from intake.md to intake-backup.md
  - Preserve instruction lines starting with `**`

**Changes:**
- `backend/storage.py` - Extract tags from first user message in `list_conversations()`
- `frontend/src/components/Sidebar.jsx` - Use tags from metadata, default to 'user' filter
- `AGENTS.md` - Added intake backup workflow

---

### v0.32.3
**Branch:** `v0.32.3`  
**Completed:** 2025-11-30 07:34 UTC | 2025-11-29 23:34 PST

**Bug Fixes:**
- **Title generation sidebar update**: Fixed WebSocket message parsing for real-time title updates
  - Title was at top level of message, not nested in `data` object
  - Sidebar now updates immediately when title generation completes
  - No manual page refresh required

**Changes:**
- `frontend/src/App.jsx` - Fixed destructuring of WebSocket title_progress message

---

### v0.32.2
**Branch:** `v0.32.2`  
**Completed:** 2025-11-30 07:04 UTC | 2025-11-29 23:04 PST

**Bug Fixes:**
- **Thinking mode auto-scroll**: Added auto-scroll to bottom while thoughts streaming
  - Automatically scrolls thinking content as new tokens arrive
  - Stops auto-scrolling if user scrolls up (respects user control)
  - Resumes auto-scrolling when user scrolls back to bottom
  - Applied to all three stage components (Stage1, Stage2, Stage3)

**Changes:**
- `frontend/src/components/Stage1.jsx` - Added thinkingRef, userScrolledRef, auto-scroll effect
- `frontend/src/components/Stage2.jsx` - Added thinkingRef, userScrolledRef, auto-scroll effect
- `frontend/src/components/Stage3.jsx` - Added thinkingRef, userScrolledRef, auto-scroll effect

---

### v0.32.1
**Branch:** `v0.32.1`  
**Completed:** 2025-11-30 06:58 UTC | 2025-11-29 22:58 PST

**Improvements:**
- **Deterministic tool call decisions**: Set temperature=0 for all tool-related inference calls
  - Classification (factual/chat/deliberation) now uses temperature=0
  - User expectations analysis uses temperature=0
  - Mid-deliberation tool assessment uses temperature=0
  - Reduces randomness in tool selection, making responses more consistent

**Changes:**
- `backend/lmstudio.py` - Added `temperature` parameter to `query_model()` and `query_model_with_retry()`
- `backend/council.py` - Updated `classify_message()`, `_analyze_user_expectations()`, `assess_tool_needs_mid_deliberation()` to use temperature=0.0

---

### v0.32.0
**Branch:** `v0.32.0`  
**Completed:** 2025-11-30 06:50 UTC | 2025-11-29 22:50 PST

**Features:**
- **Conversation Filter System (CFS) UI**: Added filter tabs to sidebar
  - Three filter groups: All, User, Test
  - User filter: excludes conversations with #auto or #test tags
  - Test filter: shows only conversations with both #auto AND #test tags
  - Parses `<!-- tags: #tag1 #tag2 | ... -->` from message content

**Changes:**
- `frontend/src/components/Sidebar.jsx` - Added CFS state, filter logic, and tab UI
- `frontend/src/components/Sidebar.css` - Added CFS tab styles

---

### v0.31.0
**Branch:** `v0.31.0`  
**Completed:** 2025-11-30 06:35 UTC | 2025-11-29 22:35 PST

**Features:**
- **Auto-test conversation tagging**: Added `--auto-test` flag to test runner
  - Prepends `<!-- tags: #auto #test | ignore this line -->` to test queries
  - Tags stored naturally in conversation JSON (no backend API changes needed)
  - Enables future frontend filtering of test vs user conversations
  - Usage: `uv run -m tests.test_runner --auto-test`

**Changes:**
- `tests/test_runner.py` - Added `--auto-test` flag and `AUTO_TEST_TAG_PREFIX` constant

---

### v0.30.13
**Branch:** `v0.30.13`  
**Completed:** 2025-11-30 06:10 UTC | 2025-11-29 22:10 PST

**Bug Fixes:**
- **Duplicate tool steps display fix**: Added unique call_id to tool call events
  - Each tool call now has a UUID-based call_id for precise tracking
  - Frontend uses call_id to prevent duplicate entries and match start/complete events
  - Allows same tool to be called multiple times without confusion

**Changes:**
- `backend/council.py` - Added call_id to tool_call_start/complete events
- `frontend/src/App.jsx` - Updated event handlers to use call_id for deduplication
- `frontend/src/components/ToolSteps.jsx` - Use call_id for React keys and filtering

---

### v0.30.12
**Branch:** `v0.30.12`  
**Completed:** 2025-11-30 06:06 UTC | 2025-11-29 22:06 PST

**Bug Fixes:**
- **MCP tool name mismatch fix**: Added cleanup of stale MCP server processes on startup
  - Kills any existing `mcp_servers` processes before starting fresh
  - Prevents port collision where old servers respond on wrong ports
  - Root cause: stale processes from previous runs causing tool discovery to hit wrong servers

**Changes:**
- `backend/mcp/registry.py` - Added `_cleanup_stale_servers()` method called during initialization

---

### v0.30.11
**Branch:** `v0.30.11`  
**Completed:** 2025-11-30 05:43 UTC | 2025-11-29 21:43 PST

**Features:**
- **Auto-warmup for unloaded models**: When LM Studio returns "model does not exist", automatically sends a quick warmup request to trigger dynamic model loading
  - Sends minimal "hi" message with max_tokens=5 to load the model
  - Tracks warmed-up models to avoid repeated warmup attempts
  - Retries original request after successful warmup

**Changes:**
- `backend/lmstudio.py` - Added `warmup_model()` function and auto-warmup logic in error handler

---

### v0.30.10
**Branch:** `v0.30.10`  
**Completed:** 2025-11-30 05:32 UTC | 2025-11-29 21:32 PST

**Bug Fixes:**
- **Improved HTTP error logging**: Added specific handling for HTTPStatusError to capture response body
  - 400 Bad Request errors now log the API response body for debugging
  - Helps diagnose model compatibility issues with LM Studio

**Changes:**
- `backend/lmstudio.py` - Added httpx.HTTPStatusError exception handler with response body logging

---

### v0.30.9
**Branch:** `v0.30.9`  
**Completed:** 2025-11-30 05:15 UTC | 2025-11-29 21:15 PST

**Bug Fixes:**
- **Classification fix for nickname/command requests**: Messages like "you shall be known as Aether" now correctly classified as "chat" instead of "deliberation"
  - Added examples for nickname requests, AI name assignments, and thank you messages
  - Expanded "chat" category description to include commands/requests to the AI

**Changes:**
- `backend/council.py` - Updated classification prompt with new examples and expanded chat definition
- `tests/scenarios.json` - Added `chat_nickname_request` test scenario

---

### v0.30.8
**Branch:** `v0.30.8`  
**Completed:** 2025-11-30 04:43 UTC | 2025-11-29 20:43 PST

**Bug Fixes:**
- **Token stream error fix**: Fixed AttributeError in MCPRegistry
  - Error: `'MCPRegistry' object has no attribute 'format_tools_for_prompt'`
  - Changed call from `format_tools_for_prompt()` to `get_tool_descriptions()`

**Changes:**
- `backend/main.py` - Fixed method name on line 714

---

### v0.30.6
**Branch:** `v0.30.6`  
**Completed:** 2025-11-30 04:20 UTC | 2025-11-29 20:20 PST

**UI Fixes:**
- **Edit message field too small**: Fixed editing form layout
  - Added flex-wrap to editing form so indicator spans full width
  - Increased min-height of textarea when editing (120px)
  - Added separator line below editing indicator

**Changes:**
- `frontend/src/components/ChatInterface.css` - Fixed input-form.editing layout

---

### v0.30.5
**Branch:** `v0.30.5`  
**Completed:** 2025-11-30 04:30 UTC | 2025-11-29 20:30 PST

**UI/UX Fixes:**
- **Message IDs moved inside frame**: IDs now appear at top-left inside message content area
- **Classification reasoning shown**: Shows reasoning text when classification completes
- **Completion message added**: Blue completion indicator at end of deliberation/direct response
- **Tool steps overlay fixed**: Removed overflow:hidden so hover overlay displays fully
- **Smart auto-scroll**: Page only auto-scrolls to bottom if user hasn't scrolled up; resets on new message

**Changes:**
- `frontend/src/components/ChatInterface.jsx`:
  - Moved message-ids inside message-content
  - Added classification-detail section with reasoning
  - Added completion-message at end of responses
  - Added userScrolledUp state for smart scroll behavior
- `frontend/src/components/ChatInterface.css`:
  - Added styles for classification-detail, completion-message
  - Adjusted message-ids positioning for inside-frame display
- `frontend/src/components/ToolSteps.css`:
  - Changed overflow:hidden to overflow:visible for proper hover overlay

---

### v0.30.4
**Branch:** `v0.30.4`  
**Completed:** 2025-11-30 04:05 UTC | 2025-11-29 20:05 PST

**Documentation:**
- **Enhanced versioning logic in AGENTS.md**: Added change classification system
  - New table for classifying change types (code fix, logic fix, UI fix, UX fix, UI addition, UX addition, feature addition)
  - New table for classifying extent (minor vs major)
  - Clear rules: only feature additions and major UI/UX additions bump feature version
  - All fixes and minor additions only bump fix version

**Changes:**
- `AGENTS.md` - Updated "Version Number Rules" section with classification tables and examples

---

### v0.30.3
**Branch:** `v0.30.3`  
**Completed:** 2025-11-30 03:30 UTC | 2025-11-29 19:30 PST

**Fixes:**
- **Memory service using wrong Graphiti tool names**: Updated to match current Graphiti MCP server API
  - Changed `graphiti.add_episode` ‚Üí `graphiti.add_memory`
  - Changed `graphiti.search_facts` ‚Üí `graphiti.search_memory_facts`
  - Cleared stale FalkorDB graphs to reset memory state

**Changes:**
- `backend/memory_service.py` - Updated tool names to match Graphiti server

---

### v0.30.2
**Branch:** `v0.30.2`  
**Completed:** 2025-11-30 02:46 UTC | 2025-11-29 18:46 PST

**Fixes:**
- **AttributeError: 'MCPRegistry' object has no attribute 'is_enabled'**: Fixed incorrect method call
  - Changed `registry.is_enabled()` to `registry.all_tools` (truthy check)
  - The `is_enabled()` method doesn't exist; `all_tools` dict is the correct way to check

**Changes:**
- `backend/main.py` - Fixed registry method call in mid-deliberation assessment

---

### v0.30.1
**Branch:** `v0.30.0` (amended)  
**Completed:** 2025-11-30 00:20 UTC | 2025-11-29 16:20 PST

**Fixes:**
- **Missing README Update for v0.30.0**: Added feature documentation to README.md
  - Added "Current Release (v0.30.0)" section with LLM-Based Tool Selection and Iterative Assessment
  - Moved previous v0.29.0 to "Previous Release"

**Documentation:**
- **Enhanced AGENTS.md Completion Checklist**: Added numbered checklist to prevent missing README updates
  - New "MANDATORY COMPLETION CHECKLIST" section with 9-step verification
  - Explicit README update requirements emphasized
  - Step 5 (Update Tracking Files) now uses checkbox format

**Changes:**
- `README.md` - Added v0.30.0 feature documentation
- `AGENTS.md` - Added mandatory completion checklist, improved Step 5 documentation

---

### v0.30.0
**Branch:** `v0.30.0`  
**Completed:** 2025-11-29 18:30 UTC | 2025-11-29 10:30 PST

**Features:**
- **LLM-Based Tool Selection**: Removed regex-based overrides for tool detection
  - Tool selection now fully relies on LLM's analysis via `_analyze_user_expectations()`
  - No more false positives from "and/or" triggering calculator
  - Simple math (5+3) handled correctly by LLM without requiring calculator tool
  
- **Iterative Tool Assessment Mid-Deliberation**: Added tool needs assessment after each stage
  - `assess_tool_needs_mid_deliberation()` evaluates if additional tools would help
  - Called after Stage 1 and Stage 2 completion
  - Only executes websearch mid-deliberation (other tools used upfront)
  - Prevents infinite loops with single-tool-per-stage limit

**Changes:**
- `backend/council.py`:
  - Removed all regex/pattern-matching overrides in `_analyze_user_expectations()`
  - Added `assess_tool_needs_mid_deliberation()` function
- `backend/main.py`:
  - Added mid-deliberation tool assessment after stage1_complete
  - Added mid-deliberation tool assessment after stage2_complete
  - New event types: `mid_deliberation_tool_start`, `mid_deliberation_tool_complete`
- `tests/scenarios.json`:
  - Updated calculator_addition to not require tool use (LLM handles simple math)
  - Disabled geolocation test (pre-existing tool name mismatch issue)

**OpenSpec:** `openspec/changes/update-tool-selection-llm-based/`

---

### v0.29.11
**Branch:** `v0.29.11`  
**Completed:** 2025-11-29 18:10 UTC | 2025-11-29 10:10 PST

**Fixes:**
- **Calculator tool incorrectly triggered by "and/or" in queries**: Fixed false positive math detection
  - "and/or" contains "/" which was being detected as a division operator
  - Changed math detection to require operators be adjacent to numbers (e.g., "5/3" not "and/or")
  - Split indicators into word-based (safe) and operator-based (regex validated)
  - Added specific check for "what is <number>" pattern

**Changes:**
- `backend/council.py` - Improved math detection in `_analyze_user_expectations()` to avoid false positives

---

### v0.29.10
**Branch:** `v0.29.10`  
**Completed:** 2025-11-29 17:49 UTC | 2025-11-29 09:49 PST

**Fixes:**
- **TypeError: Cannot read properties of undefined (reading 'length')**: Fixed crash when accessing undefined arrays
  - Added defensive check for `firstUserMessage.content?.length` in ChatInterface.jsx
  - Added null check for `output` before `JSON.stringify` in tool result display
  - Fixed all `[...prev.messages]` spreads in App.jsx to use `[...(prev?.messages || [])]`
  - Added ErrorBoundary component to catch and display React errors gracefully

**Changes:**
- `frontend/src/components/ChatInterface.jsx` - Add optional chaining for content.length, null check for output
- `frontend/src/App.jsx` - Fix all 60+ instances of spreading prev.messages without null check
- `frontend/src/ErrorBoundary.jsx` - New component for catching React errors
- `frontend/src/main.jsx` - Wrap App in ErrorBoundary

---

### v0.29.9
**Branch:** `v0.29.9`  
**Completed:** 2025-11-29 17:15 UTC | 2025-11-29 09:15 PST

**Fixes:**
- **Blank Page When Backend Unavailable**: Fixed app showing blank page instead of error screen
  - Root cause: `loadConversations` caught errors internally and returned `[]` instead of throwing
  - This prevented `initializeApp` from catching the error and setting `initError`
  - Added `throwOnError` parameter to `loadConversations` to propagate errors during init

**Changes:**
- `frontend/src/App.jsx` - Add throwOnError parameter to loadConversations, pass true during init

---

### v0.29.8
**Branch:** `v0.29.8`  
**Completed:** 2025-11-29 17:05 UTC | 2025-11-29 09:05 PST

**Fixes:**
- **Blank Page After Loading (v2)**: Enhanced error handling when backend is unavailable
  - Added `initError` state to track initialization failures
  - Shows error screen with retry button when connection fails
  - Added proper error state styling with icon and button

**Changes:**
- `frontend/src/App.jsx` - Add initError state, error screen with retry button
- `frontend/src/App.css` - Add error state styling (.init-error, .retry-btn)

---

### v0.29.7
**Branch:** `v0.29.7`  
**Completed:** 2025-11-29 17:00 UTC | 2025-11-29 09:00 PST

**Fixes:**
- **Blank Page After Loading**: Fixed app showing blank page after loading spinner
  - `initializeApp` function lacked proper error handling for unexpected errors
  - If any initialization step failed, `isInitializing` was never set to false
  - Added try-catch-finally to guarantee loading state completes even on errors

**Changes:**
- `frontend/src/App.jsx` - Wrap `initializeApp` in try-catch-finally block

---

### v0.29.6
**Branch:** `v0.29.6`  
**Completed:** 2025-11-29 16:56 UTC | 2025-11-29 08:56 PST

**Fixes:**
- **App Startup Flicker (Full Fix)**: Fixed app showing normal view briefly before loading
  - Previous fix (v0.29.5) still showed sidebar and partial UI during initialization
  - Now shows full-screen "Loading LLM Council..." spinner until initialization complete
  - Loads conversation data synchronously during init before showing any UI
  - Prevents any content from rendering until app is fully ready

**Changes:**
- `frontend/src/App.jsx` - Add `isInitializing` state, load conversation during init
- `frontend/src/App.css` - Add `.app-loading`, `.init-loading`, `.init-spinner` styles

---

### v0.29.5
**Branch:** `v0.29.5`  
**Completed:** 2025-11-29 16:52 UTC | 2025-11-29 08:52 PST

**Fixes:**
- **Conversation Load Flicker**: Fixed brief "Welcome" screen showing on app start
  - When restoring last conversation, showed empty welcome message briefly before loading
  - Now shows "Loading conversation..." spinner while conversation data is being fetched
  - Prevents confusing flicker where conversation appears then disappears

**Changes:**
- `frontend/src/App.jsx` - Pass `conversationId` prop to ChatInterface
- `frontend/src/components/ChatInterface.jsx` - Add loading state when conversation is being loaded

---

### v0.29.4
**Branch:** `v0.29.4`  
**Completed:** 2025-11-29 16:45 UTC | 2025-11-29 08:45 PST

**Fixes:**
- **Message IDs Positioning**: Changed message IDs from absolute to inline positioning
  - IDs now appear as inline elements above message content
  - Previously positioned absolutely which could overlap with other elements
  - Fixes visual overlap issues with sidebar header
  
- **Tool Steps Hover Stats**: Added stats overlay to multi-step tool calls
  - Hover over any tool step to see detailed stats (server, tool, execution time, status)
  - Shows output preview in dark theme overlay
  - Matches styling of single tool result card overlay
  - Previously only single tool results had hover overlays

**Changes:**
- `frontend/src/components/ChatInterface.css` - Changed `.message-ids` to inline positioning
- `frontend/src/components/ToolSteps.jsx` - Added hover state and stats overlay
- `frontend/src/components/ToolSteps.css` - Added stats overlay styles

---

### v0.29.3
**Branch:** `v0.29.3`  
**Completed:** 2025-11-29 15:26 UTC | 2025-11-29 07:26 PST

**Fixes:**
- **Hide Fake Placeholder Images Completely**: Fake/placeholder images now render as nothing instead of showing alt text
  - Previously showed confusing gray italic text for fake URLs
  - Now returns null, completely hiding the fake image reference
  - Provides cleaner display for old conversations with placeholder URLs

**Changes:**
- `frontend/src/components/MarkdownRenderer.jsx` - SmartImage returns null for error/fake images

---

### v0.29.2
**Branch:** `v0.29.2`  
**Completed:** 2025-11-29 15:25 UTC | 2025-11-29 07:25 PST

**Fixes:**
- **Smart Image Handling**: Broken/placeholder images now display gracefully with hover preview
  - Fake/placeholder URLs (via.placeholder.com, example.com, etc.) show alt text instead of broken icon
  - Valid images display as clickable text with camera emoji (üì∑ Alt Text)
  - Hovering over valid image text shows floating image preview tooltip
  - Fixes existing conversations that had broken image markdown
  - Preview tooltip animates in with max 400x300px size

**Changes:**
- `frontend/src/components/MarkdownRenderer.jsx` - Added SmartImage component with error detection and hover preview
- `frontend/src/components/MarkdownRenderer.css` - Added styles for image-alt-text, image-hover-container, image-preview-tooltip

---

### v0.29.1
**Branch:** `v0.29.1`  
**Completed:** 2025-11-29 15:15 UTC | 2025-11-29 07:15 PST

**Fixes:**
- **Strip Fake Placeholder Images**: Chairman/Presenter responses now strip fake/placeholder image markdown
  - Models sometimes generate fake image links like `![Image](https://via.placeholder.com/...)`
  - These render as broken image icons in the UI
  - New `strip_fake_images()` function removes placeholder URLs (via.placeholder.com, example.com, etc.)
  - All chairman prompts now include "DO NOT include images or image links" instruction
  - Applied to all stage3 synthesis functions (streaming and non-streaming)

**Changes:**
- `backend/council.py` - Added `strip_fake_images()` function, updated chairman prompts, applied to all stage3 responses

---

### v0.29.0
**Branch:** `v0.29.0`  
**Completed:** 2025-11-29 15:15 UTC | 2025-11-29 07:15 PST

**Features:**
- **Collapsible Multi-Step Tool Calls**: New UI component for displaying multiple tool calls in sequence
  - Collapsed by default with summary showing tool count and total execution time
  - Expandable header shows tool pipeline flow (e.g., "web-search ‚Üí firecrawl-scrape")
  - Individual tool steps can be expanded to show input/output details
  - Live status indicators for in-progress tool calls
  - Supports deep research workflow with multiple tool invocations
  - Works alongside existing single-tool result card (backward compatible)

**Changes:**
- `frontend/src/components/ToolSteps.jsx` - New collapsible tool steps component
- `frontend/src/components/ToolSteps.css` - Styling for tool steps accordion
- `frontend/src/components/ChatInterface.jsx` - Integrate ToolSteps component
- `frontend/src/App.jsx` - Handle `tool_call_start` and `tool_call_complete` events

---

### v0.28.0
**Branch:** `v0.28.0`  
**Completed:** 2025-11-29 15:10 UTC | 2025-11-29 07:10 PST

**Features:**
- **Enhanced Markdown Rendering**: Full support for tables, code blocks, and mermaid diagrams
  - GitHub Flavored Markdown (GFM) with tables, strikethrough, task lists
  - Syntax highlighted code blocks with language labels
  - Mermaid diagram rendering (flowcharts, sequence diagrams, etc.)
  - New shared `MarkdownRenderer` component used across all stages
  - Dependencies: remark-gfm, rehype-raw, react-syntax-highlighter, mermaid

- **Conversation/Message IDs**: Visual ID badges in message areas
  - Shows truncated conversation ID and message index in top-left corner
  - Tooltip on hover shows "Conversation ID" or "Message ID"
  - Format: `<conv_id> | <msg_index>`
  - Appears in pinned header and all message cards

**Fixes:**
- **Pinned Header Actions**: Re-run and Edit buttons now visible in pinned "Original Question" header
  - Matches functionality from v0.27.0 for inline messages

**Changes:**
- `frontend/src/components/MarkdownRenderer.jsx` - New shared component
- `frontend/src/components/MarkdownRenderer.css` - Styles for tables, code, mermaid
- `frontend/src/components/Stage1.jsx` - Use MarkdownRenderer
- `frontend/src/components/Stage2.jsx` - Use MarkdownRenderer
- `frontend/src/components/Stage3.jsx` - Use MarkdownRenderer
- `frontend/src/components/ChatInterface.jsx` - Use MarkdownRenderer, add ID badges
- `frontend/src/components/ChatInterface.css` - ID badge styles
- `frontend/package.json` - Add markdown dependencies

---

### v0.26.0
**Branch:** `v0.26.0`  
**Completed:** 2025-11-29 14:25 UTC | 2025-11-29 06:25 PST

**Features:**
- **Restore Last Conversation on Refresh**: Automatically opens the last viewed conversation
  - Persists current conversation ID to localStorage when selecting/creating conversations
  - Restores on page refresh or app restart
  - Validates stored ID exists (clears stale reference if deleted)

**Changes:**
- `frontend/src/App.jsx` - localStorage persistence for conversation ID

---

### v0.25.0
**Branch:** `v0.25.0`  
**Completed:** 2025-11-29 14:10 UTC | 2025-11-29 06:10 PST

**Features:**
- **Pinned User Message Header**: Original question stays visible when scrolling
  - Semi-transparent header appears when first user message scrolls out of view
  - Shows truncated preview (200 chars) with pin emoji
  - Backdrop blur effect for readability

- **Improved Edit/Re-run Buttons**: Better visibility and positioning
  - Buttons moved inside message card (bottom-right)
  - Added text labels ("Re-run", "Edit") alongside icons
  - Increased size and always partially visible (60% opacity on hover)
  - Styled with border separator from message content

- **Default Formatter Prompt**: Added to prompt library
  - Professional formatting with tables and emojis (conservative)
  - Creates quick takeaways and recommendations by use case
  - Adds follow-up questions at end

**Changes:**
- `frontend/src/components/ChatInterface.jsx` - Pinned header, button UI
- `frontend/src/components/ChatInterface.css` - New styles
- `data/prompt_library.json` - Add formatter_default prompt

---

### v0.24.0
**Branch:** `v0.24.0`  
**Completed:** 2025-11-29 13:30 UTC | 2025-11-29 05:30 PST

**Features:**
- **Firecrawl MCP Server**: Web scraping and content extraction
  - `firecrawl-scrape`: Scrape single URL to clean markdown
  - `firecrawl-batch-scrape`: Scrape multiple URLs (max 10)
  - Returns clean markdown with title and description
  - Uses Firecrawl API for reliable extraction

- **Deep Research Workflow**: Multi-turn research for comprehensive queries
  - Detects queries needing deep research (top N lists, comparisons, rankings)
  - Step 1: Web search for relevant sources
  - Step 2: LLM identifies most relevant URLs from results
  - Step 3: Firecrawl extracts content from selected pages
  - Step 4: Combines content for council deliberation
  - Enables better answers for "top 10 EVs", "best laptops", etc.

**Changes:**
- `mcp_servers/firecrawl/` - New Firecrawl MCP server
- `mcp_servers.json` - Add firecrawl server config
- `backend/council.py` - Add deep research workflow functions

---

### v0.23.3
**Branch:** `v0.23.3`  
**Completed:** 2025-11-29 09:50 UTC | 2025-11-29 01:50 PST

**Fixes:**
- **Tool Stats Overlay Visibility**: Fix overlay not appearing on hover
  - Changed `overflow: hidden` to `overflow: visible` on tool card
  - Increased z-index to 1000 for proper stacking

- **DateTime Timezone Detection**: Datetime tool now auto-detects timezone from IP
  - Enhanced `system_date_time` server to call ipinfo.io for timezone
  - Returns formatted string with timezone: "Saturday, November 29, 2025 at 01:43 AM (America/Los_Angeles)"
  - Includes user location in response
  - Updated clean output extraction to use pre-formatted string

**Changes:**
- `frontend/src/components/Stage1.css` - Fix overlay visibility
- `mcp_servers/system_date_time/server.py` - Add timezone detection
- `backend/council.py` - Update datetime output extraction

---

### v0.23.2
**Branch:** `v0.23.2`  
**Completed:** 2025-11-29 09:25 UTC | 2025-11-29 01:25 PST

**Features:**
- **Tool Call Stats Overlay**: Hover over purple MCP tool card to see detailed stats
  - Shows server name, tool name, execution time, success status
  - Displays full output (truncated to 500 chars)
  - Dark theme overlay with smooth hover animation

**Changes:**
- `frontend/src/components/ChatInterface.jsx` - Add stats overlay to tool card
- `frontend/src/components/Stage1.css` - Add overlay styles

---

### v0.23.1
**Branch:** `v0.23.1`  
**Completed:** 2025-11-29 07:00 UTC | 2025-11-28 23:00 PST

**Fixes:**
- **Classification Badge Flicker**: Fix badge showing "Deliberation" briefly during classifying
  - Show "Classifying" until classification.status is explicitly "complete"
  
- **Clean Tool Output Formatting**: Tool responses no longer include raw JSON
  - New `_extract_clean_tool_output()` function formats tool data for humans
  - Calculator: "5 + 3 = 8" instead of JSON structure
  - DateTime: "Date and time: 2025-11-29 14:30:00 (local time)"
  - Clearer extraction prompts with explicit "do not include raw data" rules

- **Prompt Engineering Improvements**: Cleaner direct responses
  - Updated default extraction prompts for all categories
  - Added explicit formatting rules to avoid duplicate/verbose output
  - Cleared cached prompts to use new improved prompts

**Changes:**
- `frontend/src/components/ChatInterface.jsx` - Fix classification badge logic
- `backend/council.py` - Add `_extract_clean_tool_output()`, improve prompt
- `backend/prompt_library.py` - Update default extraction prompts
- `tests/scenarios.json` - Adjust min_length for calculator tests

---

### v0.23.0
**Branch:** `v0.23.0`  
**Completed:** 2025-11-29 06:35 UTC | 2025-11-28 22:35 PST

**Features:**
- **MCP Tool Execution Time**: Display execution time for MCP tool calls
  - Track execution time in `mcp/registry.py` `call_tool()` method
  - Include `execution_time_seconds` in tool result events
  - Display execution time in tool result card header (e.g., "2.5s")
  - Styled with subtle badge in header

**Changes:**
- `backend/mcp/registry.py` - Track and return execution time
- `backend/council.py` - Include execution_time_seconds in tool_result event
- `frontend/src/App.jsx` - Store executionTime in toolResult
- `frontend/src/components/ChatInterface.jsx` - Display execution time in tool card
- `frontend/src/components/Stage1.css` - Style for tool-time badge

---

### v0.22.7
**Branch:** `v0.22.7`  
**Completed:** 2025-11-29 06:30 UTC | 2025-11-28 22:30 PST

**Fixes:**
- **Title Regeneration on Edit**: Editing the first message now regenerates the title
  - Added `regenerate_title` flag to API request model
  - `handleEditMessage` sets `regenerateTitle: true` when editing first message (index 0)
  - Backend includes `regenerate_title` in `needs_title` condition
  - Title reflects new content after editing first message

**Changes:**
- `frontend/src/App.jsx` - Pass `regenerateTitle` flag when editing first message
- `frontend/src/api.js` - Add `regenerateTitle` parameter to `sendMessageStreamTokens`
- `backend/main.py` - Add `regenerate_title` field to request model, update title condition

---

### v0.22.6
**Branch:** `v0.22.6`  
**Completed:** 2025-11-29 06:25 UTC | 2025-11-28 22:25 PST

**Fixes:**
- **Title Generation on Rerun**: Conversations with generic titles now regenerate titles on message rerun
  - Changed condition from `is_first_message && generic_title` to just `needs_title`
  - `needs_title = current_title.startswith("Conversation ") or not current_title`
  - Added `title_complete` event handler in `runCouncilForMessage`
  - Fixed in both streaming and non-streaming endpoints

**Changes:**
- `backend/main.py` - Update title generation condition in both endpoints
- `frontend/src/App.jsx` - Add `title_complete` handler to `runCouncilForMessage`

---

### v0.22.5
**Branch:** `v0.22.5`  
**Completed:** 2025-11-29 06:22 UTC | 2025-11-28 22:22 PST

**Fixes:**
- **Classification Badge Shows Immediately**: Badge now shows "üîç Classifying..." as soon as message is sent
  - Initialize `classification: { status: 'classifying' }` when creating assistant message
  - Applies to both `handleSendMessage` and `runCouncilForMessage`

- **News/Current Events Override**: Added deterministic override for news queries
  - Detects queries with news indicators (this week, today, latest, news, events, headlines)
  - Combined with time-sensitive words (week, today, 2024, 2025, current, recent)
  - Forces `needs_external_data: true` and `data_types_needed: ["news"]`
  - Ensures websearch is used for "What major events happened this week?"

**Changes:**
- `frontend/src/App.jsx` - Initialize classification status on message creation
- `backend/council.py` - Add news/current events override in expectation analysis

---

### v0.22.4
**Branch:** `v0.22.4`  
**Completed:** 2025-11-29 06:10 UTC | 2025-11-28 22:10 PST

**Fixes:**
- **Hide Duplicates from Main List**: Duplicate conversations are now only shown in the collapsible Duplicates section
  - Main conversation list filters out duplicate IDs
  - Uses `filteredConversations` computed from `duplicateInfo`
  - Keeps first/newest conversation in main list, shows rest in Duplicates section

**Changes:**
- `frontend/src/components/Sidebar.jsx` - Filter duplicates from main list

---

### v0.22.3
**Branch:** `v0.22.3`  
**Completed:** 2025-11-29 06:05 UTC | 2025-11-28 22:05 PST

**Fixes:**
- **Title Generation for Default Titles**: Fixed title generation not working at startup
  - `_needs_title_generation()` now works with metadata-only conversation dicts
  - Handles both `messages` array and `message_count` property
  - Excludes deleted conversations from title generation
  - Excludes duplicate conversations from title generation queue

**Features:**
- **Duplicates Collapsible Section**: Added collapsible section showing duplicate conversations
  - Purple collapsible section above "Clean Duplicates" button
  - Shows duplicate groups with first query preview
  - Click to navigate to any duplicate conversation
  - Collapsed by default

**Changes:**
- `backend/title_service.py` - Fixed `_needs_title_generation()`, exclude duplicates
- `frontend/src/components/Sidebar.jsx` - Added duplicates section
- `frontend/src/components/Sidebar.css` - Added duplicates styling

---

### v0.22.2
**Branch:** `v0.22.2`  
**Completed:** 2025-11-29 05:58 UTC | 2025-11-28 21:58 PST

**Fixes:**
- **Duplicate Cleanup Fix**: Fixed `soft_delete_conversation is not defined` error
  - Added `soft_delete_conversation()` function to `storage.py`
  - Function marks conversation as deleted with timestamp
  - Used by `delete_duplicate_conversations()` for soft deletion

**Changes:**
- `backend/storage.py` - Added `soft_delete_conversation()` function

---

### v0.22.0
**Branch:** `v0.22.0`  
**Completed:** 2025-11-29 05:50 UTC | 2025-11-28 21:50 PST

**Features:**
- **Duplicate Conversation Cleanup**: UI to find and delete duplicate conversations
  - New backend endpoints:
    - `GET /api/conversations/duplicates` - Find duplicate conversation groups
    - `POST /api/conversations/duplicates/delete` - Delete duplicates, keep newest
  - Sidebar "Clean Duplicates" button (appears when duplicates exist)
  - Shows count of removable duplicates
  - Confirms before deletion, keeps newest copy of each group

**Technical Details:**
- `find_duplicate_conversations()` - Groups conversations by MD5 hash of user queries
- `delete_duplicate_conversations()` - Soft-deletes duplicates, keeps one per group
- Frontend fetches duplicate info on mount and when conversations change

**Changes:**
- `backend/storage.py` - Added duplicate detection and deletion functions
- `backend/main.py` - Added API endpoints for duplicates
- `frontend/src/components/Sidebar.jsx` - Added cleanup button and logic
- `frontend/src/components/Sidebar.css` - Added button styling

---

### v0.21.3
**Branch:** `v0.21.3`  
**Completed:** 2025-11-29 05:45 UTC | 2025-11-28 21:45 PST

**Fixes:**
- **Title Generation at Startup**: Fixed conversations with default titles ("Conversation <id>") not being regenerated
  - Added `initialize_title_service()` call during app startup
  - Title service now scans for untitled conversations and queues them for generation
  - Added `shutdown_title_service()` call during app shutdown for clean cleanup
  - Import both `title_service` functions and instance from separate modules

**Technical Details:**
- `title_service.py` contains the full TitleGenerationService with background worker
- `title_generation.py` contains the simpler TitleGenerationService instance
- Both are now properly imported and used in main.py

**Changes:**
- `backend/main.py` - Initialize title service at startup, shutdown at exit

---

### v0.21.2
**Branch:** `v0.21.2`  
**Completed:** 2025-11-29 03:35 UTC | 2025-11-28 19:35 PST

**Fixes:**
- **Calculator Tool Routing**: Fixed "5 plus 3" not using calculator MCP tool
  - Added deterministic `_parse_calculator_query()` function for reliable number extraction
  - Maps data type "calculation" to just "calculator" server (not specific operation)
  - Phase 2 now uses fast path for calculator with regex-based number parsing
  - Supports: plus/add, minus/subtract, times/multiply, divided/divide operations
  - Avoids LLM unreliability in parsing simple numbers

- **Math Query Detection Override**: Added post-processing fix for expectation analysis
  - LLM sometimes says simple math doesn't need tools (treating it as "general knowledge")
  - Added deterministic override that forces `needs_external_data: true` for any query with numbers and math keywords
  - Keywords: plus, minus, times, divided, multiply, add, subtract, calculate, compute, +, -, *, /, etc.

**Technical Details:**
- Tool execution works correctly (verified: a=5, b=3 ‚Üí result=8)
- Response generation sometimes hallucinates wrong numbers (LLM issue, not tool issue)
- Test updated: `calculator_addition` with query "What is 5 plus 3?"

**Changes:**
- `backend/council.py` - Added `_parse_calculator_query()`, math detection override
- `tests/scenarios.json` - Updated calculator_addition test, fixed factual_capital min_length

---

### v0.21.1
**Branch:** `v0.21.1`  
**Completed:** 2025-11-29 03:10 UTC | 2025-11-28 19:10 PST

**Documentation:**
- **README.md Update**: Added v0.21.0 feature documentation (was missing)
  - Added "Current Release (v0.21.0)" section for LLM-Based Tool Confidence Routing
  - Moved previous v0.20.0 to "Previous Release" section

- **AGENTS.md Improvements**: Made README updates more prominent to prevent future misses
  - Added warning symbols and "DO NOT SKIP" emphasis
  - Added README Update Checklist before commit
  - Added Step 5 "Update Documentation" in Versioning Process (before commit step)
  - Made README update a mandatory step with verification checklist

**Changes:**
- `README.md` - Added v0.21.0 feature documentation
- `AGENTS.md` - Enhanced README update instructions

---

### v0.21.0
**Branch:** `v0.21.0`  
**Completed:** 2025-11-29 03:00 UTC | 2025-11-28 19:00 PST

**Features:**
- **LLM-Based Tool Confidence Routing**: Replaced hardcoded keyword lists with intelligent LLM-based system
  - Step 1: LLM analyzes user expectations and determines data types needed
  - Step 2: Deterministic mapping from data types to appropriate tools
  - More flexible and extensible than keyword matching

**Architecture:**
- New `_analyze_user_expectations()`: LLM call to extract user expectations and data type needs
- New `_evaluate_tool_confidence()`: Fast deterministic data-type-to-tool mapping
- Removed: `WEBSEARCH_KEYWORDS`, `GEOLOCATION_KEYWORDS`, `_requires_websearch()`, `_requires_geolocation()`
- Added: `TOOL_CONFIDENCE_THRESHOLD` (0.5) for tool selection

**Data Type Mappings:**
- `current_time` ‚Üí system-date-time.get-system-date-time (95% confidence)
- `location` ‚Üí system-geo-location.get-system-geo-location (95% confidence)
- `news`, `weather`, `current_events` ‚Üí websearch.search (90% confidence)
- `calculation` ‚Üí calculator tools (85% confidence)
- `web_content` ‚Üí retrieve-web-page.get-page-from-url (80% confidence)

**Benefits:**
- Single LLM call for expectation analysis (vs two LLM calls in pure LLM approach)
- Fast, deterministic tool selection based on data types
- More maintainable than keyword lists
- Extensible - add new data types and tools easily

**Test Results:** All 6 test scenarios pass (100%)

**Changes:**
- `backend/council.py` - New expectation-based tool routing system
- `backend/main.py` - Always run tool check (LLM handles no-tool cases)

---

### v0.20.4
**Branch:** `v0.20.4`  
**Completed:** 2025-11-29 02:45 UTC | 2025-11-28 18:45 PST

**Fixes:**
- **Geolocation Tool Fast-Path**: Added keyword-based fast-path for geolocation queries
  - "What's my address?" now correctly triggers the geo-location tool
  - Added `GEOLOCATION_KEYWORDS` list similar to `WEBSEARCH_KEYWORDS`
  - Added `_requires_geolocation()` function for keyword detection
  - Updated `_phase1_analyze_query()` with geolocation fast-path
  - Updated `needs_tool_check` condition in main.py

**Keywords Added:**
- `my location`, `my address`, `where am i`, `what's my address`
- `my ip`, `my city`, `my country`, `located`, `my postal`
- `my zip code`, `my region`, `geolocation`, `ip address`
- `what city`, `which city`, `what state`, `which state`
- `what country`, `which country`

**Additional:**
- Added refusal phrases for "don't have access to personal"
- Enabled geolocation test scenario in scenarios.json

**Changes:**
- `backend/council.py` - Added GEOLOCATION_KEYWORDS and _requires_geolocation()
- `backend/main.py` - Added geolocation check to needs_tool_check condition
- `tests/scenarios.json` - Updated geolocation test scenario

---

### v0.20.3
**Branch:** `v0.20.3`  
**Completed:** 2025-11-29 01:45 UTC | 2025-11-28 17:45 PST

**Improvements:**
- **Graphiti Edge Extraction Logging**: Reduced log noise for missing entity warnings
  - Changed log level from WARNING to DEBUG for "Invalid entity IDs in edge extraction"
  - Added new `patch_edge_logging.py` patch script
  - Updated Dockerfile to apply patch during build
  - Edges with missing entities still skipped, but logs are cleaner

**Technical Details:**
- The issue occurs when LLM edge extraction references entities not in the entity list
- This is expected behavior with local LLMs due to inconsistent entity/edge extraction
- Full multi-turn extraction would require significant upstream changes
- Current solution: Clean logs + skip invalid edges (same behavior, less noise)

**Changes:**
- `mcp_servers/graphiti-custom/patch_edge_logging.py` - New patch script
- `mcp_servers/graphiti-custom/Dockerfile` - Added patch execution
- `mcp_servers/graphiti-custom/README.md` - Updated documentation

---

### v0.20.2
**Branch:** `v0.20.2`  
**Completed:** 2025-11-29 01:25 UTC | 2025-11-28 17:25 PST

**Documentation:**
- **Graphiti Edge Extraction Warning**: Documented upstream graphiti_core warning
  - "Invalid entity IDs in edge extraction" is gracefully handled (edge skipped)
  - Caused by LLM inconsistency in entity vs edge extraction
  - No fix needed - this is expected behavior with local LLMs
  - Updated `mcp_servers/graphiti-custom/README.md` with Known Warnings section

**Changes:**
- `mcp_servers/graphiti-custom/README.md` - Added Known Warnings documentation

---

### v0.20.1
**Branch:** `v0.20.1`  
**Completed:** 2025-11-29 01:05 UTC | 2025-11-28 17:05 PST

**Features:**
- **Graphiti Test Runner Script**: New `run_graphiti_test.sh` for running Graphiti tests
  - Automatically starts/restarts Graphiti MCP server with latest config
  - Waits for server health before running tests
  - Reports test results with helpful diagnostics

**Fixes:**
- **Separate Test Database**: Graphiti tests now use `test_graphiti` group_id
  - Prevents test data from conflicting with production LLM Council data
  - LLM Council uses `llm_council` group_id, tests use `test_graphiti`

- **Test Scenarios Updated**: Enabled more test scenarios and fixed assertions
  - `current_news_websearch` - Verified working
  - `current_date_tool` - Verified working  
  - `factual_capital` - Fixed min_length assertion
  - `chat_greeting` - Verified working
  - `real_time_data_awareness` - Verified working

**Changes:**
- `run_graphiti_test.sh` - New test runner script
- `tests/test_graphiti.py` - Changed group_id to `test_graphiti`
- `tests/scenarios.json` - Enabled core tests, fixed assertions

---

### v0.20.0
**Branch:** `v0.20.0`  
**Completed:** 2025-11-29 00:45 UTC | 2025-11-28 16:45 PST

**Features:**
- **Graphiti Memory Integration**: Persistent memory with confidence-based fast-path responses
  - New `backend/memory_service.py` module for Graphiti knowledge graph interaction
  - Records all messages (user, council members, chairman) to memory asynchronously
  - Memory-based response path when confidence exceeds threshold (default 80%)
  - Configurable confidence model in `config.json` (defaults to chairman if not specified)
  - Age-weighted confidence scoring for memory freshness (max_memory_age_days configurable)
  - Graceful degradation when Graphiti unavailable - standard workflow continues
  - New `/api/memory/status` endpoint for memory service status

**Changes:**
- `backend/memory_service.py` - New memory service module with MemoryService class
- `backend/main.py` - Added memory initialization, check, and recording hooks
- `backend/config_loader.py` - Added `get_memory_config()` and `get_confidence_model()`
- `config.json` - Added `memory` section and `confidence` model configuration
- `README.md` - Updated Key Features section with v0.20.0 release info
- `openspec/changes/add-graphiti-memory-integration/` - Change proposal

**Configuration:**
```json
{
  "models": {
    "confidence": {
      "id": "",
      "name": "Memory Confidence Scorer",
      "description": "Model for scoring memory relevance (empty = use chairman)"
    }
  },
  "memory": {
    "enabled": true,
    "confidence_threshold": 0.8,
    "max_memory_age_days": 30,
    "group_id": "llm_council",
    "record_user_messages": true,
    "record_council_responses": true,
    "record_chairman_synthesis": true
  }
}
```

---

### v0.19.2
**Branch:** `v0.19.2`  
**Completed:** 2025-11-29 00:05 UTC | 2025-11-28 16:05 PST

**Fixes:**
- **Enforce Project tmp/ Folder Usage**: Added explicit rule in AGENTS.md to always use project's `tmp/` folder
  - Never use system `/tmp/` directory
  - Documents correct paths: `tmp/test_results/`, `tmp/intake.md`
  - Prevents temporary files from being created outside project

**Changes:**
- `AGENTS.md` - Added "Critical Rules" section with tmp folder guidance

---

### v0.19.1
**Branch:** `v0.19.1`  
**Completed:** 2025-11-28 23:54 UTC | 2025-11-28 15:54 PST

**Fixes:**
- **Prevent Direct Master Commits**: Added git pre-commit hook to enforce versioning workflow
  - Hook blocks commits to master/main with helpful error message
  - Warns when branch name doesn't follow `v<release>.<feature>.<fix>` format
  - New `setup-dev.sh` script installs hooks after cloning
  - Updated AGENTS.md with hook documentation
  - Updated README with setup instructions

**Changes:**
- `.git/hooks/pre-commit` - Git hook (local, installed via setup script)
- `setup-dev.sh` - Development environment setup script
- `AGENTS.md` - Added Git Hook Protection section
- `README.md` - Added Step 0 for dev environment setup

---

### v0.19.0
**Branch:** `v0.19.0`  
**Completed:** 2025-11-28 23:52 UTC | 2025-11-28 15:52 PST

**Features:**
- **JSON Configuration for Graphiti MCP**: Added centralized JSON-based configuration system
  - New `graphiti_config.json` as main config file for LLM, embedder, database settings
  - New `generate_config.py` script to generate `config.yaml` and `.env` from JSON
  - Updated `start.sh` to auto-regenerate config on startup
  - Updated README with full documentation and troubleshooting guide

**Changes:**
- `mcp_servers/graphiti-custom/graphiti_config.json` - Main configuration file
- `mcp_servers/graphiti-custom/generate_config.py` - Config generator script
- `mcp_servers/graphiti-custom/start.sh` - Updated to use JSON config
- `mcp_servers/graphiti-custom/README.md` - Full documentation update
- `mcp_servers/graphiti-custom/.gitignore` - Added .env

---

### v0.15.5
**Branch:** `v0.15.5`  
**Completed:** 2025-11-28 09:50 UTC | 2025-11-28 01:50 PST

**Fixes:**
- **Tool Failure Honest Reporting**: Fixed test validation and model behavior when MCP tools fail internally
  - Added `evaluate_tool_success()` to test framework - checks tool output content for success/error
  - Added `_tool_output_failed()` helper to detect tool failures from output JSON
  - Updated `format_tool_result_for_prompt()` to detect failures and instruct honest reporting
  - Updated prompts to tell models NOT to fabricate data when tools fail
  - Tests now correctly fail when tool returns error in output (previously passed incorrectly)
  - Models now honestly report "tool failed" instead of making up fake data

**Changes:**
- `backend/council.py` - Tool failure detection, honest failure prompts
- `tests/test_runner.py` - Tool success validation in test evaluator
- `openspec/changes/fix-tool-failure-honest-reporting.md` - OpenSpec proposal

---

### v0.15.4
**Branch:** `v0.15.4`  
**Completed:** 2025-11-28 09:25 UTC | 2025-11-28 01:25 PST

**Fixes:**
- **LLM Tool Awareness**: Fixed models refusing to use MCP tool data claiming "cannot access real-time information"
  - Strengthened system prompts with explicit instructions to use tool output as factual data
  - Added refusal phrase detection with automatic retry using stronger prompts
  - Enhanced tool result formatting with timestamps and "LIVE DATA" emphasis
  - Added weather-related keywords to websearch trigger list
  - Fixes: Models now correctly use websearch/tool results instead of claiming training data limitations

**Changes:**
- `backend/council.py` - Refusal detection, retry logic, stronger prompts, enhanced tool formatting
- `openspec/changes/fix-llm-tool-awareness/` - OpenSpec proposal documentation

---

### v0.15.3
**Branch:** `v0.15.3`  
**Completed:** 2025-11-28 08:45 UTC | 2025-11-28 00:45 PST

**Fixes:**
- **Non-streaming API Routing**: Added intelligent message classification and routing to non-streaming endpoint
  - `/api/conversations/{id}/message` now classifies messages before processing
  - Simple/factual queries route to direct response (skip council deliberation)
  - Tool usage works for both direct and deliberation response paths
  - Significantly faster responses for factual questions and chat
  - Fixes: test framework timeout issues, real-time data queries now work in non-streaming API

**Changes:**
- `backend/main.py` - Complete rewrite of send_message endpoint with classification and routing
- `tests/test_runner.py` - Updated to properly handle direct_response in result
- `tests/scenarios.json` - Added 'core' tag for critical tests, adjusted timeouts
- `openspec/AGENTS.md` - Added auto-implementation instructions after proposal validation

---

### v0.15.1
**Branch:** `v0.15.1`  
**Completed:** 2025-11-28 07:15 UTC | 2025-11-27 23:15 PST

**Fixes:**
- **Message Re-run Cleanup**: Fixed duplicate messages appearing when re-running a conversation
  - Added `truncate_at` and `skip_user_message` params to API
  - Backend truncates messages at specified index before re-run
  - Frontend passes truncateAt for redo/edit message operations
  - Fixes: re-running messages no longer leaves stale responses
  
- **Websearch URL Format**: Fixed SearXNG search endpoint compatibility
  - Changed URL from `?q="{query}"` to `/search?q={query}` format
  - Spaces now replaced with `+` instead of URL encoding
  - Added HTML result parsing (extracts title, URL, snippet from SearXNG response)
  - Fixes: websearch MCP tool now returns proper search results

**Files Changed:**
- `backend/main.py` - Added truncate_at/skip_user_message to SendMessageRequest
- `frontend/src/App.jsx` - Pass truncateAt to API for redo/edit
- `frontend/src/api.js` - Added truncateAt, skipUserMessage params
- `mcp_servers/websearch/server.py` - Fixed URL format, added HTML parsing

---

### v0.8.5
**Branch:** `v0.8.5-test-infrastructure`  
**Completed:** 2025-11-28 07:00 UTC | 2025-11-27 23:00 PST

**Fixes:**
- **Title Service Method Error**: Fixed 500 Internal Server Error when sending first message
  - Removed undefined `get_title_service()` call - service is already imported
  - Fixed `generate_title_immediate` ‚Üí `generate_title` method name
  - Fixes: send_message endpoint crashing on first conversation message

**Features:**
- **Automated Test Infrastructure**: Added comprehensive test framework
  - `tests/test_runner.py` - Full test runner with server lifecycle management
  - `tests/test_mcp_tools.py` - Quick MCP tool validation script
  - `tests/scenarios.json` - Configurable test scenarios
  - Auto-starts/stops server for automated testing
  - Support for MCP direct tool calls and full council tests

**Files Changed:**
- `backend/main.py` - Fixed title service method calls
- `tests/test_runner.py` - New automated test runner
- `tests/test_mcp_tools.py` - New MCP tool tests (8 tests, 100% pass rate)
- `tests/scenarios.json` - Test scenario definitions
- `tests/__init__.py` - Package marker

---

### v0.8.3
**Branch:** `v0.8.3`  
**Completed:** 2025-11-28 05:40 UTC | 2025-11-27 21:40 PST

**Fixes:**
- **MCP Websearch Not Triggered for News Queries**: Fixed tool detection for current events
  - Added keyword-based pre-detection (news, latest, trending, headlines, etc.)
  - Short-circuit to websearch.search when keywords detected
  - Strengthened Phase 1 analysis prompt with explicit tool routing rules
  - Clarified that model HAS access to external tools
  - Fixes: "what are today's top 5 news" now correctly uses websearch tool

**Files Changed:**
- `backend/council.py` - Added WEBSEARCH_KEYWORDS, _requires_websearch(), updated _phase1_analyze_query()

---

### v0.14.1
**Branch:** `v0.14.1`  
**Completed:** 2025-11-28 05:35 UTC | 2025-11-27 21:35 PST

**Fixes:**
- **TokenTracker Definition Order**: Fixed NameError preventing backend startup
  - TokenTracker class was used before its definition in council.py
  - Moved class definition from line 299 to immediately after imports (line 20)
  - Fixes: `NameError: name 'TokenTracker' is not defined`

**Files Changed:**
- `backend/council.py` - Moved TokenTracker class to top of file

---

### v0.14.0
**Branch:** `v0.14.0`  
**Completed:** 2025-11-28 05:21 UTC | 2025-11-27 21:21 PST

**Features:**
- **Formatter Model for Direct Responses**: Added two-step response generation for direct responses
  - Chairman model first generates the response content
  - Formatter model (if configured and different) improves formatting
  - New frontend event handlers for formatter streaming (formatter_start, formatter_token, formatter_thinking, formatter_complete)
  - Falls back to chairman model if formatter not configured or same as chairman
  - Config supports separate formatter model with full connection parameters

**Files Changed:**
- `backend/council.py` - Added `_apply_formatter()` function and formatter integration in `chairman_direct_response()`
- `frontend/src/App.jsx` - Added formatter event handlers in both WebSocket processing paths
- `config.json` - Set formatter model to qwen/qwen3-4b-thinking-2507

---

### v0.8.2
**Branch:** `v0.8.2`  
**Completed:** 2025-11-28 04:49 UTC | 2025-11-27 20:49 PST

**Fixes:**
- **Classification Status Display**: Fixed "Analyzing..." status persisting after response completes
  - Classification status now properly updates to 'complete' in direct_response_complete handler
  - Classification status now properly updates to 'complete' in stage3_complete handler
  - Both streaming path and standard path fixed

- **MCP Date Context**: Fixed MCP tools returning outdated dates (e.g., "July 2024")
  - Added current date/time context to tool analysis prompt in _phase1_analyze_query
  - Model now receives actual current date/time for time-sensitive tool decisions
  - Added time context to chairman_direct_response for consistent date awareness

**Files Changed:**
- `frontend/src/App.jsx` - Updated completion handlers
- `backend/council.py` - Added datetime import and time context

---

### v0.8.1
**Branch:** `v0.8.1`  
**Completed:** 2025-11-28 03:59 UTC | 2025-11-27 19:59 PST

**Fixes:**
- **Overlay Hover Behavior**: Fixed issue where overlays would close when cursor briefly moved over title area and back into overlay
  - Added unified hover tracking for title-area element
  - Both title-area and overlay containers now properly maintain hover state
  - Prevents accidental closure during normal mouse movement

**Changes:**
- Updated `handleTitleAreaEnter` to set both hover states
- Unified title-area hover tracking to prevent premature overlay closure

---

### v0.13.0
**Branch:** `v0.13.0`  
**Completed:** 2025-11-28 03:55 UTC | 2025-11-27 19:55 PST

**Features:**
- **Overlay Hover Delay**: 2-second delay before MCP overlays close
  - Allows smooth navigation between MCP server overlay and tools overlay
  - Timer cancels when cursor re-enters any overlay in the group
  - Both the server list and tools overlays are part of the same group

**Changes:**
- Added `overlayCloseTimerRef` for managing close delay
- Added `handleOverlayGroupEnter` and `handleOverlayGroupLeave` functions
- Applied overlay group handlers to title-area, mcp-overlay, and mcp-tools-overlay
- Removed immediate close on server item mouse leave

---

### v0.12.0
**Branch:** `v0.12.0`  
**Completed:** 2025-11-28 03:45 UTC | 2025-11-27 19:45 PST

**Features:**
- **MCP Status Indicator**: Visual MCP server status in sidebar header
  - "MCP" badge next to "LLM Council" title (blue bold with grey edge)
  - Hover overlay showing all MCP servers with status indicators
  - Red (offline), yellow (busy), green (available) status colors
  - Secondary hover overlay showing server tools
  - Tool in-use highlighting (yellow) for active tools
  - Metrics display: server count, tool count, active tools

**Changes:**
- Enhanced `backend/mcp/registry.py` with server/tool status tracking
- Added `getMcpStatus()` API function in `frontend/src/api.js`
- Updated `frontend/src/components/Sidebar.jsx` with MCP overlay UI
- Added 145+ lines of CSS for overlay styling

---

### v0.11.0
**Branch:** `change-022-system-timezone-mcp-server`  
**Completed:** 2025-11-28 02:55 UTC | 2025-11-27 18:55 PST

**Features:**
- **system-timezone-mcp**: New MCP server for retrieving timezone information
  - Tool: `get-timezone-list` with no input parameters
  - Returns: System timezone (based on IP) and complete list of tz database timezones
  - Parses Wikipedia's List of tz database time zones page
  - 24-hour caching for timezone list (data rarely changes)
  - Fallback to minimal timezone list if Wikipedia unavailable

**Changes:**
- Created `mcp_servers/system_timezone/` directory with server.py
- Added server to `mcp_servers.json` configuration
- Server uses http_wrapper for HTTP/stdio dual mode

---

### v0.10.0
**Branch:** `change-021-system-geo-location-mcp-server`  
**Completed:** 2025-11-28 01:40 UTC | 2025-11-27 17:40 PST

**Features:**
- **system-geo-location-mcp**: New MCP server for retrieving system location based on IP
  - Tool: `get-system-geo-location` with no input parameters
  - Returns: City, State/Region, Postal Code, Country
  - Uses whatismyip.com for IP-based geolocation
  - No external API keys required

**Changes:**
- Created `mcp_servers/system_geo_location/` directory with server.py
- Added server to `mcp_servers.json` configuration
- Server uses http_wrapper for HTTP/stdio dual mode

---

### v0.9.0
**Branch:** `change-020-retrieve-web-page-mcp-server`  
**Completed:** 2025-11-27 22:05 UTC | 2025-11-27 14:05 PST

**Features:**
- **retrieve-web-page-mcp**: New MCP server for fetching HTML content from URLs
  - Tool: `get-page-from-url` with URL input parameter
  - Returns success status, URL, content, and HTTP status code
  - Handles HTTP errors, URL errors, and timeouts (30s)
  - User-Agent: `Mozilla/5.0 (compatible; LLMCouncil/1.0)`
  - Foundation for geo-location and timezone MCP servers

**Changes:**
- Created `mcp_servers/retrieve_web_page/` directory with server.py
- Added server to `mcp_servers.json` configuration
- Server uses http_wrapper for HTTP/stdio dual mode

---

### v0.8.0
**Branch:** `v0.8.0`  
**Completed:** 2025-11-27 18:45 UTC | 2025-11-27 10:45 PST

**Features:**
- **intelligent-message-routing**: Chairman classifies messages before deliberation
  - Phase 0: Classification - chairman evaluates message type (factual/chat/deliberation)
  - Direct response path for factual questions and casual chat (skips council stages)
  - Full 3-stage deliberation for complex questions requiring opinions/comparison/feedback
  - Visual classification badges in UI showing response type
  - Blue-themed styling for direct responses vs green for deliberated answers

**Changes:**
- `backend/council.py`: Added `classify_message()` and `chairman_direct_response()` functions
- `backend/main.py`: Routing logic in streaming endpoint based on classification
- `frontend/src/App.jsx`: Event handlers for classification and direct response events
- `frontend/src/components/ChatInterface.jsx`: Conditional rendering based on response type
- `frontend/src/components/Stage3.jsx`: Support for `isDirect` prop with different styling
- `frontend/src/components/ChatInterface.css`: Classification badge styling
- `frontend/src/components/Stage3.css`: Direct response styling (blue theme)
- `openspec/changes/change-023-intelligent-message-routing.md`: Proposal document

---

### v0.7.0
**Branch:** `v0.7.0`  
**Completed:** 2025-11-27 17:29 UTC | 2025-11-27 09:29 PST

**Features:**
- **system-date-time-mcp**: New MCP server for getting system date and time
  - Tool: `get-system-date-time` with `return_type` parameter
  - Formats: `time` (HH:MM:SS), `date` (YYYY-MM-DD), `both` (full datetime), `unix` (timestamp)
  - Returns structured data with individual components (year, month, day, hour, minute, second, weekday, ISO format)

**Changes:**
- Created `mcp_servers/system_date_time/` directory with server.py
- Added server to `mcp_servers.json` configuration
- Server runs on dynamic port (base_port + index)

---

### v0.6.4
**Branch:** `v0.6.4`  
**Completed:** 2025-11-27 17:00 UTC | 2025-11-27 09:00 PST

**Features:**
- **intelligent-mcp-tool-calling**: Two-phase LLM-driven tool detection and execution
  - Phase 1: Analysis - tool_calling model analyzes queries against detailed MCP server info
  - Phase 2: Execution - generates specific tool calls with correct parameters
  - Comprehensive tool info includes server details, tool descriptions, parameters, types, and allowed values
  - Removes hardcoded keyword matching in favor of intelligent LLM analysis
  - Better tool selection accuracy for math, web search, and future MCP capabilities

**Changes:**
- `backend/council.py`: Refactored `check_and_execute_tools()` into two-phase approach
  - Added `_extract_json_from_response()` helper for robust JSON parsing
  - Added `_phase1_analyze_query()` for intelligent query analysis
  - Added `_phase2_generate_tool_call()` for tool call generation
- `backend/mcp/registry.py`: Added `get_detailed_tool_info()` for comprehensive tool documentation
  - Simplified `should_use_tools()` to defer to LLM analysis

---

### v0.6.3
**Branch:** `v0.6.3`  
**Completed:** 2025-11-27 16:30 UTC | 2025-11-27 08:30 PST

**Features:**
- **mcp-server-dynamic-ports**: HTTP transport for MCP servers
  - MCP servers now run on HTTP ports starting at 15000
  - Sequential port assignment: calculator=15000, websearch=15001, etc.
  - Added `/health` endpoint for server readiness checks
  - HTTP JSON-RPC replaces stdio communication
  - Enables better monitoring, debugging, and external tool integration

**Changes:**
- `mcp_servers.json`: Added `base_port` (15000) and optional `port` per server
- `mcp_servers/http_wrapper.py`: New HTTP server wrapper for MCP servers
- `mcp_servers/calculator/server.py`: Uses http_wrapper for HTTP/stdio dual mode
- `mcp_servers/websearch/server.py`: Uses http_wrapper for HTTP/stdio dual mode
- `backend/mcp/client.py`: Added HTTP transport support alongside stdio
- `backend/mcp/registry.py`: Dynamic port assignment and port tracking

---

### v0.6.2
**Branch:** `v0.6.2`  
**Completed:** 2025-11-27 12:15 UTC | 2025-11-27 04:15 PST

**Features:**
- **websearch-mcp**: Web search capability via MCP server
  - Local search endpoint integration at `http://127.0.0.1:8080?q="<search>"`
  - Automatic detection of time-sensitive topics, current events, specific entities
  - Search results formatted and injected into council context
  - Keywords: current, latest, news, price, weather, who is, etc.

**Changes:**
- Created `mcp_servers/websearch/` with server.py implementing MCP protocol
- Updated `mcp_servers.json` to include websearch server
- Updated `backend/mcp/registry.py` with search indicators in `should_use_tools()`

---

### v0.6.1
**Branch:** `v0.6.1`  
**Completed:** 2025-11-27 11:35 UTC | 2025-11-27 03:35 PST

**Features:**
- **tool-calling-model-config**: Dedicated tool_calling model configuration for MCP
  - Added `tool_calling` model section in config.json
  - Set `nexveridian/granite-4.0-h-tiny` as default tool-calling model
  - Separates MCP tool decisions from council reasoning models
  - Allows use of faster/smaller models optimized for function calling

**Changes:**
- `config.json`: Added tool_calling model configuration section
- `backend/config_loader.py`: Added `get_tool_calling_model()` function and model connection resolution
- `backend/council.py`: Updated to use tool_calling model for MCP tool decisions

---

### v0.6.0
**Branch:** `v0.6.0`  
**Completed:** 2025-01-27 10:20 UTC | 2025-01-27 02:20 PST

**Features:**
- **mcp-capability**: Model Context Protocol (MCP) integration for extensible tool usage
  - MCP client/registry for discovering and managing MCP servers
  - Calculator MCP server for basic math operations (add, subtract, multiply, divide)
  - Automatic tool detection based on query content
  - Tool usage displayed with server name, capability, input/output in Stage 1
  - Purple-themed tool result card in UI

**Changes:**
- Created `backend/mcp/` module with client.py and registry.py
- Created `mcp_servers/calculator/` with server.py implementing MCP protocol
- Added `mcp_servers.json` configuration file
- Updated `backend/main.py` with MCP initialization at startup, shutdown, and API endpoints
- Updated `backend/council.py` with tool detection and execution in stage1_collect_responses_streaming
- Updated `frontend/src/components/Stage1.jsx` to display tool results
- Updated `frontend/src/components/Stage1.css` with tool result card styling
- Updated `frontend/src/App.jsx` to handle tool_result events

---

### v0.5.2
**Branch:** `v0.5.2`  
**Completed:** 2025-11-27 09:57 UTC | 2025-11-27 01:57 PST

**Fixes:**
- **persist-timer-display**: Timer and tokens/s now persist after streaming completes
  - Previously, badges disappeared when `isStreaming` turned false
  - Frontend now shows badges whenever values exist, not just during streaming
  - Backend sends final timing info (thinking_seconds, elapsed_seconds) in model complete events
  - Added `get_final_timing()` method to TokenTracker class

**Changes:**
- `backend/council.py`: Added `get_final_timing()` method, included timing in all complete events
- `frontend/src/App.jsx`: Updated 6 model_complete handlers to capture final timing values
- `frontend/src/components/Stage1.jsx`: Show tps-badge and timing-badge without isStreaming check
- `frontend/src/components/Stage2.jsx`: Show tps-badge and timing-badge without isStreaming check
- `frontend/src/components/Stage3.jsx`: Show tps-badge and timing-badge without isStreaming check

---

### v0.5.1
**Branch:** `v0.5.1`  
**Completed:** 2025-11-27 09:51 UTC | 2025-11-27 01:51 PST

**Fixes:**
- **tokens-per-second-thinking**: Fixed tokens/s not displaying during thinking phase
  - Backend now sends `tokens_per_second` in all thinking events
  - Thinking tokens are counted toward overall TPS calculation
  - Frontend preserves TPS value when updating thinking state
- **console-spam-redux**: Additional cleanup of WebSocket console spam
  - Removed remaining print statements from WebSocket handler
  - Silenced client ID tracking for cleaner logs

**Changes:**
- `backend/council.py`: Extended `record_thinking()` to accept delta and return TPS
- `backend/council.py`: Added `tokens_per_second` to stage1/2/3 thinking events
- `frontend/src/App.jsx`: Added `tokensPerSecond` to all 6 thinking event handlers
- `backend/main.py`: Removed client_id prints from WebSocket handler
- `start.sh`: Already had `--log-level warning` from v0.4.1

---

### v0.5.0
**Branch:** `v0.5.0`  
**Completed:** 2025-11-27 09:44 UTC | 2025-11-27 01:44 PST

**Features:**
- **response-timing-display**: Real-time stopwatch showing thinking and total response time
  - Displays `<thinking_seconds>s/<total_seconds>s` format next to tokens/s
  - Shows timing in tab headers during streaming
  - Separate timing tracking for thinking vs response phases
  - Orange badge styling to differentiate from tok/s display

**Changes:**
- `backend/council.py`: Extended TokenTracker class with timing methods (record_thinking, mark_thinking_done, get_timing)
- `backend/council.py`: Added timing data to all stage token/thinking events
- `frontend/src/App.jsx`: Pass timing data (thinkingSeconds, elapsedSeconds) to streaming state
- `frontend/src/components/Stage1.jsx`: Display timing badge with formatTiming helper
- `frontend/src/components/Stage2.jsx`: Display timing badge with formatTiming helper
- `frontend/src/components/Stage3.jsx`: Display timing badge with formatTiming helper
- `frontend/src/components/Stage1.css`: Added timing-badge and timing-indicator styles
- `frontend/src/components/Stage2.css`: Added timing-badge and timing-indicator styles
- `frontend/src/components/Stage3.css`: Added timing-badge style

---

### v0.4.1
**Branch:** `v0.4.1`  
**Completed:** 2025-01-27 09:30 UTC | 2025-01-27 01:30 PST

**Fixes:**
- **console-spam**: Removed WebSocket connection/disconnection log messages
  - Silenced client connect/disconnect prints in backend
  - Set uvicorn log level to 'warning' in start.sh
- **tokens-display**: Fixed tokens/s not displaying during streaming
  - Removed `> 0` condition that hid initial values
  - Now shows tok/s only while actively streaming
  - Formatted to one decimal place for consistency

**Changes:**
- `backend/main.py`: Removed print statements from WebSocket handler
- `start.sh`: Added `--log-level warning` to uvicorn command
- `frontend/src/components/Stage1.jsx`: Fixed tps display conditions
- `frontend/src/components/Stage2.jsx`: Fixed tps display conditions
- `frontend/src/components/Stage3.jsx`: Fixed tps display conditions

---

### v0.4.0
**Branch:** `v0.4.0`  
**Completed:** 2025-01-27 08:52 UTC | 2025-01-27 00:52 PST

**Features:**
- **multi-round-deliberation**: Quality-based multi-round streaming deliberation
  - Stage 2 now runs multiple rounds if any response rated <30%
  - Each round: models rank AND rate (1-5) responses with feedback
  - Low-rated responses are refined based on peer feedback
  - Respects `max_rounds` config setting
- **tokens-per-second**: Real-time tokens/s display during streaming
  - Shows tok/s in tab headers and model labels
  - Updates in real-time as tokens stream
  - Final tok/s shown on completion
- **round-progress**: Round indicator in Stage 2 header
  - Shows "Round X / Y" during multi-round deliberation
  - Indicates "(Refinement)" for refinement rounds
  - Hidden for single-round deliberation

**Changes:**
- `backend/council.py`: Added TokenTracker class, quality rating extraction, multi-round streaming
- `backend/main.py`: Handle new stage2 return format with deliberation metadata
- `frontend/src/components/Stage1.jsx`: Display tokens/s
- `frontend/src/components/Stage2.jsx`: Display tokens/s and round indicator
- `frontend/src/components/Stage3.jsx`: Display tokens/s
- `frontend/src/App.jsx`: Handle round_start/round_complete events and tps data
- `frontend/src/components/*.css`: Styling for tps badges and round indicator

---

### v0.3.10
**Branch:** `v0.3.10`  
**Completed:** 2025-01-27 08:22 UTC | 2025-01-27 00:22 PST

**Fixes:**
- **remove-max-tokens**: Remove max_tokens limits causing truncation
  - Reasoning models use tokens for thinking AND response
  - 1024 max_tokens was cutting off responses mid-thought
  - Set all stages to `null` (unlimited, use model defaults)

**Changes:**
- `config.json`: `max_tokens` for stage1/2/3 set to `null`

---

### v0.3.9
**Branch:** `v0.3.9`  
**Completed:** 2025-01-27 08:15 UTC | 2025-01-27 00:15 PST

**Fixes:**
- **streaming-no-timeout**: Remove read timeout for streaming entirely
  - Reasoning models can pause for unlimited time between tokens
  - `read=None` allows stream to complete naturally
- **evaluation-timeout**: Increase evaluation timeout to 600s
  - Background evaluation uses council models (reasoning)
  - Was timing out at 120s
- **no-retries**: Set `max_retries: 0` to avoid wasted time
  - Long timeouts make retries unnecessary

**Changes:**
- `query_model_streaming()` now uses `read=None` (no timeout)
- All timeouts now 600s including evaluation
- `max_retries` set to 0

---

### v0.3.8
**Branch:** `v0.3.8`  
**Completed:** 2025-01-27 07:55 UTC | 2025-01-26 23:55 PST

**Fixes:**
- **timeout-increase-v2**: Increased all timeouts to 10 minutes (600s)
  - Reasoning models can think for 5+ minutes between chunks
  - `streaming_chunk_timeout`: 600s (new config)
  - `default_timeout`: 600s
  - `connection_timeout`: 30s (was 10s)
  - `evaluation_timeout`: 120s (was 60s)
  - `max_retries`: 1 (was 2, since timeout is longer)

**Changes:**
- `query_model()` now loads timeout from config instead of hardcoded 30s
- `query_model_streaming()` now loads timeout from config instead of hardcoded 300s
- All timeout functions use config values with sensible fallbacks

---

### v0.3.7
**Branch:** `v0.3.7`  
**Completed:** 2025-01-27 07:25 UTC | 2025-01-26 23:25 PST

**Fixes:**
- **empty-content-fallback**: Use reasoning_content when content is empty
  - Some models output everything in reasoning_content field
  - Stage1 and Stage3 now fallback to reasoning_content
- **streaming-timeout**: Fixed streaming timeout configuration
  - Read timeout now properly set for per-chunk waiting

**Changes:**
- Updated `council.py` stage1 and stage3 to use reasoning_content fallback
- Fixed httpx timeout config in `lmstudio.py` streaming

---

### v0.3.6
**Branch:** `v0.3.6`  
**Completed:** 2025-01-27 07:05 UTC | 2025-01-26 23:05 PST

**Fixes:**
- **timeout-increase**: Increased default timeout from 30s to 300s (5 minutes)
  - Reasoning models need more time for inference
  - Prevents repeated retries when model still thinking
  - Reduced max_retries from 3 to 2

**Changes:**
- Updated `config.json` timeout_config:
  - `default_timeout`: 30 ‚Üí 300
  - `council_timeout`: 300 (new)
  - `chairman_timeout`: 300 (new)
  - `title_generation_timeout`: 60 ‚Üí 300
  - `evaluation_timeout`: 60 (new, shorter for eval)
  - `max_retries`: 3 ‚Üí 2
- Added `for_evaluation` parameter to `query_model_with_retry()`
- Updated all timeout references in council.py

---

### v0.3.5
**Branch:** `v0.3.5`  
**Completed:** 2025-01-27 06:48 UTC | 2025-01-26 22:48 PST

**Fixes:**
- **cleanup-invalid-models**: Removed test-model and test-model-2 from metrics
- **evaluator-selection**: Fixed evaluator to never be same as target model
  - Only uses valid council members or chairman
  - Uses second-best if target is highest-rated

**Changes:**
- Added `cleanup_invalid_models()` to `model_metrics.py`
- Added `get_valid_models()` and `get_evaluator_for_model()`
- Cleanup runs at server startup
- Each response evaluated by best available model (excluding itself)

---

### v0.3.4
**Branch:** `v0.3.4`  
**Completed:** 2025-01-27 06:07 UTC | 2025-01-26 22:07 PST

**Features:**
- **metrics-markdown**: Auto-generate `data/llm_metrics.md` alongside JSON
  - Rankings table with model, rating, success rate
  - Detailed scores for each model
  - Updates whenever metrics are recorded

**Changes:**
- Added `_save_metrics_markdown()` to `model_metrics.py`
- `save_metrics()` now generates both JSON and markdown

---

### v0.3.3
**Branch:** `v0.3.3`  
**Completed:** 2025-01-27 05:52 UTC | 2025-01-26 21:52 PST

**Fixes:**
- **metrics-logging**: Confirmed llm_metrics.json IS working (file in data/ folder)
- Added debug logging to metrics evaluation flow

**Features:**
- **markdown-export**: Final council answer saved as markdown file
  - Format: `<conversation_title>__utc_date_time.md`
  - Saved in project root (data/ parent folder)
  - Contains user query and final answer

**Changes:**
- Added `save_final_answer_markdown()` to `storage.py`
- Integrated markdown export into streaming endpoint
- Enhanced metrics evaluation logging for debugging

---

### v0.3.2
**Branch:** `v0.3.2`  
**Completed:** 2025-01-27 05:35 UTC | 2025-01-26 21:35 PST

**Fixes:**
- **metrics-file-path**: Fixed llm_metrics.json path to use data/ directory (was using cwd)
- **redo-duplicate-message**: Redo no longer creates duplicate user message

**Changes:**
- `model_metrics.py` now uses absolute path in `data/` directory
- Refactored `handleRedoMessage` and `handleEditMessage` to use `runCouncilForMessage()`
- New `runCouncilForMessage()` function runs council without adding user message

---

### v0.3.1
**Branch:** `v0.3.1`  
**Completed:** 2025-01-27 05:04 UTC | 2025-01-26 21:04 PST

**Fixes:**
- **retry-blank-responses**: Council members retry up to 2 times on blank/failed responses

**Features:**
- **model-quality-metrics**: Auto-evaluate responses to build quality metrics per model
  - Categories: verbosity, expertise, adherence, clarity, overall
  - Persistent storage in `llm_metrics.json`
  - Model ranking based on composite rating
  - API endpoints: `/api/metrics`, `/api/metrics/ranking`

**Changes:**
- Added `backend/model_metrics.py` for metrics tracking
- Retry logic in `stream_model()` with max 2 retries
- Background evaluation after Stage 1 responses
- New `stage1_model_retry` event for UI notification

---

### v0.3.0
**Branch:** `v0.3.0`  
**Completed:** 2025-01-27 04:17 UTC | 2025-01-26 20:17 PST

**Features:**
- **rich-formatting**: Enhanced Presenter prompt for tables, headers, code blocks, structured output
- **redo-message**: Redo icon (‚Üª) to re-run council with same message
- **edit-message**: Edit icon (‚úé) to modify and resubmit message

**Changes:**
- Updated stage3 prompts to request rich markdown formatting
- Renamed "Chairman" to "Presenter" in UI for final answer stage
- Added message action buttons (redo/edit) on hover for user messages
- Added editing state with cancel option and visual indicator
- New handlers in App.jsx: `handleRedoMessage`, `handleEditMessage`

---

### v0.2.0
**Branch:** `v0.2.0`  
**Completed:** 2025-01-27 03:15 UTC | 2025-01-26 19:15 PST

**Fixes:**
- **app-title**: Changed web app title from "frontend" to "LLM Council (local)"

**Features:**
- **follow-up-input**: Input field now appears after Final Council Answer for follow-up questions
- **formatter-model**: Optional formatter LLM for Final Council Answer (falls back to chairman)

**Changes:**
- Updated `frontend/index.html` title
- Modified `ChatInterface.jsx` to show input form after stage3 completes
- Added `formatter` model config in `config.json` (empty = use chairman)
- Added `get_formatter_model()` in `config_loader.py`
- Updated `council.py` to use FORMATTER_MODEL for stage3 synthesis

---

### v0.1.0
**Branch:** `v0.1.0`  
**Completed:** 2025-01-27 01:25 UTC | 2025-01-26 17:25 PST

**Features:**
- **add-title-tooltip**: Conversation title tooltip on hover in sidebar
- **add-streaming-deliberation**: Real-time token streaming to UI for all deliberation stages
- **add-reactive-streaming**: Concise prompts and configurable max_tokens for faster responses

**Changes:**
- Added `query_model_streaming()` async generator for token-level streaming
- New `/api/conversations/{id}/message/stream-tokens` endpoint
- Frontend streaming state management with live content updates
- Thinking sections with collapsible display for reasoning models
- Visual streaming indicators (blinking cursor, progress badges)
- `response_config` in config.json for response style and token limits

---

## Format Reference

```markdown
### v<version>
**Branch:** `v<version>`  
**Completed:** YYYY-MM-DD HH:MM UTC | YYYY-MM-DD HH:MM <local-tz>

**Features:** (new capabilities)
- **feature-name**: Brief description

**Fixes:** (bug fixes)
- **fix-name**: Brief description

**Changes:** (implementation details)
- Technical detail 1
- Technical detail 2
```
