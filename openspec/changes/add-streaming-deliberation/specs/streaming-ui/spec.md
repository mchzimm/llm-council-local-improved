## ADDED Requirements

### Requirement: Real-time Token Streaming
The system SHALL stream LLM response tokens to the web UI in real-time as they are generated by each model.

#### Scenario: Stage 1 response streaming
- **WHEN** a council model generates response tokens during Stage 1
- **THEN** each token is streamed to the UI and displayed progressively in the model's response tab

#### Scenario: Stage 2 evaluation streaming
- **WHEN** a council model generates evaluation tokens during Stage 2
- **THEN** each token is streamed to the UI and displayed progressively in the model's evaluation tab

#### Scenario: Stage 3 synthesis streaming
- **WHEN** the chairman model generates synthesis tokens during Stage 3
- **THEN** each token is streamed to the UI and displayed progressively in the final answer section

### Requirement: Reasoning Model Thinking Display
The system SHALL display thinking/reasoning tokens from reasoning models as they are generated, separate from the main response content.

#### Scenario: Thinking tokens during generation
- **WHEN** a reasoning model emits thinking tokens before its response
- **THEN** the thinking tokens are displayed in a distinct thinking section that updates in real-time

#### Scenario: Completed thinking display
- **WHEN** a reasoning model completes its thinking phase
- **THEN** the thinking section remains visible and the response section begins streaming

### Requirement: Streaming Progress Indicator
The system SHALL display a visual indicator when content is actively streaming from a model.

#### Scenario: Active streaming indication
- **WHEN** tokens are being received from a model
- **THEN** a visual indicator (such as a blinking cursor or animation) shows streaming is in progress

#### Scenario: Streaming complete indication
- **WHEN** a model finishes generating its response
- **THEN** the streaming indicator is removed and the content is displayed as complete
